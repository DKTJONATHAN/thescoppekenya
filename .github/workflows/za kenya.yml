name: Kenya News British Gossip Scraper (Google News + Internal Links)

on:
Â  schedule:
Â  Â  - cron: '0 3 * * *'
Â  Â  - cron: '30 4 * * *'
Â  Â  - cron: '0 6 * * *'
Â  Â  - cron: '30 7 * * *'
Â  Â  - cron: '0 9 * * *'
Â  Â  - cron: '30 10 * * *'
Â  Â  - cron: '0 12 * * *'
Â  Â  - cron: '30 13 * * *'
Â  Â  - cron: '0 15 * * *'
Â  Â  - cron: '30 16 * * *'
Â  Â  - cron: '0 18 * * *'
Â  Â  - cron: '30 19 * * *'
Â  workflow_dispatch:
Â  Â  inputs:
Â  Â  Â  manual_topic:
Â  Â  Â  Â  description: 'Force topic: entertainment, politics, news, sports, business, tech'
Â  Â  Â  Â  required: false
Â  Â  Â  Â  default: ''

concurrency:
Â  group: ${{ github.workflow }}-${{ github.ref }}
Â  cancel-in-progress: false

permissions:
Â  contents: write

env:
Â  DEFAULT_BRANCH: main
Â  POSTS_DIR: content/posts
Â  MEMORY_FILE: .github/scrape_memory.json

jobs:
Â  generate-and-publish:
Â  Â  runs-on: ubuntu-latest
Â  Â  steps:
Â  Â  Â  - name: Checkout repo
Â  Â  Â  Â  uses: actions/checkout@v4
Â  Â  Â  Â  with:
Â  Â  Â  Â  Â  fetch-depth: 0
Â  Â  Â  Â  Â  ref: ${{ env.DEFAULT_BRANCH }}
Â  Â  Â  Â  Â  token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
Â  Â  Â  Â  Â  persist-credentials: true

Â  Â  Â  - name: Set up Python
Â  Â  Â  Â  uses: actions/setup-python@v5
Â  Â  Â  Â  with:
Â  Â  Â  Â  Â  python-version: '3.11'

Â  Â  Â  - name: Install dependencies
Â  Â  Â  Â  run: |
Â  Â  Â  Â  Â  python -m pip install --upgrade pip
Â  Â  Â  Â  Â  pip install requests google-genai beautifulsoup4 lxml playwright
Â  Â  Â  Â  Â  playwright install chromium

Â  Â  Â  - name: Generate article with Gemini
Â  Â  Â  Â  env:
Â  Â  Â  Â  Â  GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
Â  Â  Â  Â  Â  GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
Â  Â  Â  Â  Â  UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
Â  Â  Â  Â  Â  MANUAL_TOPIC: ${{ inputs.manual_topic }}
Â  Â  Â  Â  Â  POSTS_DIR: ${{ env.POSTS_DIR }}
Â  Â  Â  Â  Â  MEMORY_FILE: ${{ env.MEMORY_FILE }}
Â  Â  Â  Â  run: |
Â  Â  Â  Â  Â  python << 'EOF'
Â  Â  Â  Â  Â  import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap, itertools
Â  Â  Â  Â  Â  from google import genai
Â  Â  Â  Â  Â  from google.genai import types
Â  Â  Â  Â  Â  from bs4 import BeautifulSoup
Â  Â  Â  Â  Â  from playwright.sync_api import sync_playwright

Â  Â  Â  Â  Â  today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
Â  Â  Â  Â  Â  full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
Â  Â  Â  Â  Â  current_hour = datetime.datetime.utcnow().hour

Â  Â  Â  Â  Â  MODELS_TO_TRY = ["gemini-3-flash-preview", "gemini-2.5-flash"]

Â  Â  Â  Â  Â  BANNED_PHRASES = [
Â  Â  Â  Â  Â  Â  Â  "sasa basi", "melting the pot", "spill the tea", "tea is hot",Â 
Â  Â  Â  Â  Â  Â  Â  "grab your popcorn", "listen up", "buckle up", "breaking news",
Â  Â  Â  Â  Â  Â  Â  "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
Â  Â  Â  Â  Â  Â  Â  "mambo vipi", "niaje wasee", "karibuni", "tapestry", "dive in",
Â  Â  Â  Â  Â  Â  Â  "delve into", "moreover", "furthermore", "in conclusion",Â 
Â  Â  Â  Â  Â  Â  Â  "it's worth noting", "a testament to", "navigating the landscape",
Â  Â  Â  Â  Â  Â  Â  "in today's digital age", "shed light on"
Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  hour_to_topic = {
Â  Â  Â  Â  Â  Â  Â  3: 'entertainment', 4: 'news', 6: 'entertainment', 7: 'politics',
Â  Â  Â  Â  Â  Â  Â  9: 'entertainment', 10: 'sports', 12: 'entertainment', 13: 'business',
Â  Â  Â  Â  Â  Â  Â  15: 'entertainment', 16: 'tech', 18: 'entertainment', 19: 'news'
Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
Â  Â  Â  Â  Â  valid_topics = ['entertainment', 'politics', 'news', 'sports', 'business', 'tech']

Â  Â  Â  Â  Â  if manual_input in valid_topics:
Â  Â  Â  Â  Â  Â  Â  topic = manual_input
Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  topic = hour_to_topic.get(current_hour, 'entertainment')

Â  Â  Â  Â  Â  print(f"ğŸ¯ Topic: {topic}")

Â  Â  Â  Â  Â  base_keywords = {
Â  Â  Â  Â  Â  Â  Â  'entertainment': 'kenya entertainment OR kenya celebrity OR nairobi gossip OR kenya showbiz',
Â  Â  Â  Â  Â  Â  Â  'politics': 'kenya politics OR kenya parliament OR nairobi politics OR kenya government',
Â  Â  Â  Â  Â  Â  Â  'news': 'kenya breaking news OR nairobi news OR kenya latest news',
Â  Â  Â  Â  Â  Â  Â  'sports': 'kenya sports OR kenya athletics OR kenya football OR nairobi sports',
Â  Â  Â  Â  Â  Â  Â  'business': 'kenya economy OR kenya business OR kenya finance OR nairobi markets',
Â  Â  Â  Â  Â  Â  Â  'tech': 'kenya tech OR kenya technology OR nairobi startup OR kenya innovation'
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  query_str = base_keywords.get(topic, 'kenya breaking news')
Â  Â  Â  Â  Â  safe_query = urllib.parse.quote(f"{query_str} when:2d")

Â  Â  Â  Â  Â  rss_url = f"https://news.google.com/rss/search?q={safe_query}&hl=en-KE&gl=KE&ceid=KE:en"
Â  Â  Â  Â  Â  print(f"ğŸŒ Fetching Google News: {rss_url}")

Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  resp = requests.get(rss_url, timeout=15)
Â  Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(resp.content, "xml")
Â  Â  Â  Â  Â  Â  Â  articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")]
Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  print(f"Error fetching Google News: {e}")
Â  Â  Â  Â  Â  Â  Â  sys.exit(0)

Â  Â  Â  Â  Â  if not articles:
Â  Â  Â  Â  Â  Â  Â  print("No fresh news, skipping")
Â  Â  Â  Â  Â  Â  Â  sys.exit(0)

Â  Â  Â  Â  Â  memory_path = os.environ.get('MEMORY_FILE')
Â  Â  Â  Â  Â  memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

Â  Â  Â  Â  Â  def scrape_article_free(url):
Â  Â  Â  Â  Â  Â  Â  print(f"ğŸ•·ï¸ Trying full article from Google News: {url}")
Â  Â  Â  Â  Â  Â  Â  with sync_playwright() as p:
Â  Â  Â  Â  Â  Â  Â  Â  Â  browser = p.chromium.launch(headless=True, args=[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--no-sandbox",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--disable-dev-shm-usage",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "--disable-blink-features=AutomationControlled"
Â  Â  Â  Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  Â  Â  Â  context = browser.new_context(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  viewport={"width": 1920, "height": 1080}
Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  page = context.new_page()
Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.goto(url, timeout=60000, wait_until="domcontentloaded")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any(x in page.url for x in ["google.com", "news.google.com"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("Â  Â Still on Google wrapper -> forcing direct link...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  direct = page.evaluate('''() => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const links = Array.from(document.querySelectorAll('a[href^="http"]'));
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (let a of links) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const h = a.href;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (!h.includes('google.com') && !h.includes('news.google.com') && h.length > 40) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return h;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return null;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }''')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if direct:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  Â Found direct link -> {direct[:80]}...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.goto(direct, timeout=60000, wait_until="networkidle")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.click('text=/Visit|Continue|Go to site/i', timeout=10000)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.wait_for_timeout(6000)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.wait_for_timeout(8000)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.wait_for_timeout(4000)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âœ… Landed on real article: {page.url}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = page.content()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(content, "html.parser")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  article_text = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  containers = soup.select('article, main, .article-body, .post-content, .entry-content, .story-content, .content-body, .article__content, .post-body')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for c in containers:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  paras = c.find_all('p')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chunk = "\n\n".join([p.get_text(strip=True) for p in paras if len(p.get_text(strip=True)) > 40])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(chunk) > 600:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  article_text = chunk
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(article_text) < 600:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_paras = soup.find_all('p')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chunk = "\n\n".join([p.get_text(strip=True) for p in all_paras if len(p.get_text(strip=True)) > 40])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(chunk) > 400:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  article_text = chunk

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(article_text) < 400:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  article_text = soup.get_text(separator='\n\n', strip=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chunks = [c.strip() for c in article_text.split('\n\n') if len(c.strip()) > 80]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if chunks:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  article_text = max(chunks, key=len)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img_url = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for meta in soup.find_all('meta'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if meta.get('property') in ['og:image', 'twitter:image']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img_url = meta.get('content', '')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if img_url: break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  Â Extracted {len(article_text)} characters of text")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return article_text, img_url

Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  Â Scrape failed: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return None, None
Â  Â  Â  Â  Â  Â  Â  Â  Â  finally:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  browser.close()

Â  Â  Â  Â  Â  full_raw_text, final_hash, target_title, target_image = None, None, None, None
Â  Â  Â  Â  Â  for a in articles:
Â  Â  Â  Â  Â  Â  Â  u_hash = hashlib.md5(a['url'].encode()).hexdigest()
Â  Â  Â  Â  Â  Â  Â  if u_hash in memory: continue
Â  Â  Â  Â  Â  Â  Â  result = scrape_article_free(a['url'])
Â  Â  Â  Â  Â  Â  Â  if not result: continue
Â  Â  Â  Â  Â  Â  Â  text, img = result
Â  Â  Â  Â  Â  Â  Â  if text and len(text) > 300:
Â  Â  Â  Â  Â  Â  Â  Â  Â  full_raw_text = text
Â  Â  Â  Â  Â  Â  Â  Â  Â  target_image = img
Â  Â  Â  Â  Â  Â  Â  Â  Â  final_hash = u_hash
Â  Â  Â  Â  Â  Â  Â  Â  Â  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  breakÂ 

Â  Â  Â  Â  Â  if not full_raw_text:Â 
Â  Â  Â  Â  Â  Â  Â  print("âŒ No valid unread content found.")
Â  Â  Â  Â  Â  Â  Â  sys.exit(0)

Â  Â  Â  Â  Â  def get_internal_context(posts_dir):
Â  Â  Â  Â  Â  Â  Â  if not os.path.exists(posts_dir): return ""
Â  Â  Â  Â  Â  Â  Â  files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
Â  Â  Â  Â  Â  Â  Â  if not files: return ""
Â  Â  Â  Â  Â  Â  Â  sample = random.sample(files, min(len(files), 3))
Â  Â  Â  Â  Â  Â  Â  context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY IN THE TEXT USING MARKDOWN):\n"
Â  Â  Â  Â  Â  Â  Â  for f in sample:
Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  txt = c.read()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_match = re.search(r'title:\s*"(.*?)"', txt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t = t_match.group(1) if t_match else f
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context += f"- Title: {t} | Link: https://zandani.co.ke/posts/{f.replace('.md', '')}\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  except: continue
Â  Â  Â  Â  Â  Â  Â  return context

Â  Â  Â  Â  Â  def get_real_image(query):
Â  Â  Â  Â  Â  Â  Â  access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
Â  Â  Â  Â  Â  Â  Â  fallback = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
Â  Â  Â  Â  Â  Â  Â  if not access_key: return fallback
Â  Â  Â  Â  Â  Â  Â  url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  resp = requests.get(url, timeout=10)
Â  Â  Â  Â  Â  Â  Â  Â  Â  if resp.status_code == 200: return resp.json()['urls']['regular']
Â  Â  Â  Â  Â  Â  Â  except: pass
Â  Â  Â  Â  Â  Â  Â  return fallback

Â  Â  Â  Â  Â  def dash_scrubber(text):
Â  Â  Â  Â  Â  Â  Â  if not text: return ""
Â  Â  Â  Â  Â  Â  Â  text = text.replace('\u2014', '-').replace('\u2013', '-')
Â  Â  Â  Â  Â  Â  Â  text = text.replace(chr(8212), '-').replace(chr(8211), '-')
Â  Â  Â  Â  Â  Â  Â  return text

Â  Â  Â  Â  Â  # --- API KEY ROTATION LOGIC ---
Â  Â  Â  Â  Â  raw_keys = [os.environ.get("GEMINI_WRITE_KEY"), os.environ.get("GEMINI_API_KEY")]
Â  Â  Â  Â  Â  api_keys = [k for k in raw_keys if k and k.strip()]
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  if not api_keys:
Â  Â  Â  Â  Â  Â  Â  print("âŒ No Gemini API keys provided in environment.")
Â  Â  Â  Â  Â  Â  Â  sys.exit(1)

Â  Â  Â  Â  Â  key_cycle = itertools.cycle(api_keys)
Â  Â  Â  Â  Â  current_key = next(key_cycle)
Â  Â  Â  Â  Â  client = genai.Client(api_key=current_key)

Â  Â  Â  Â  Â  google_search_tool = types.Tool(google_search=types.GoogleSearch())
Â  Â  Â  Â  Â  internal_links_data = get_internal_context(os.environ.get("POSTS_DIR", "content/posts"))

Â  Â  Â  Â  Â  prompt = f'''Current Date: {full_date_str}
Â  Â  Â  Â  Â  NEWS HEADLINE: "{target_title}"
Â  Â  Â  Â  Â  SOURCE SCRAPED TEXT: "{full_raw_text[:12000]}"
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  {internal_links_data}

Â  Â  Â  Â  Â  TASK: Write a news report as a **British Tabloid Gossip Columnist**.
Â  Â  Â  Â  Â  1. Use Google Search to research the latest details on this story if needed.
Â  Â  Â  Â  Â  2. Naturally insert a markdown link to one of the PAST STORIES provided above somewhere in the body paragraph.
Â  Â  Â  Â  Â  3. Identify the main topic and issues discussed in the scraped article. Then, generate a new article that is 100% related but focuses on aspects not covered in the original (e.g., what the story hasn't talked about). Reuse the main keywords from the original headline naturally and heavily in the new TITLE (front-loaded), EXCERPT, and every subheading you create for better SEO.

Â  Â  Â  Â  Â  LANGUAGE MANDATE (STRICT):
Â  Â  Â  Â  Â  - NO SHENG OR SWAHILI. You are writing for an audience who loves simple, dramatic British gossip.
Â  Â  Â  Â  Â  - Use very simple, conversational British English. Think of a chatty, cheeky tabloid reporter having a chinwag.
Â  Â  Â  Â  Â  - Keep the vocabulary basic but the tone highly dramatic and gossipy.Â 
Â  Â  Â  Â  Â  - Use conversational fillers naturally to sound human: "Right,", "Well,", "Honestly,", "Anyway,", "Mate."
Â  Â  Â  Â  Â  - Include rhetorical questions and subjective asides in brackets (e.g., "Can you even imagine?", "What a nightmare.").
Â  Â  Â  Â  Â  - Vary sentence length aggressively. Mix short, punchy facts with longer, breathy gossip thoughts.
Â  Â  Â  Â  Â  - BANNED PHRASES: {", ".join(BANNED_PHRASES)}.
Â  Â  Â  Â  Â  - NEVER use long dashes or double dashes. Only use regular single hyphens.

Â  Â  Â  Â  Â  ANTI-REPETITION RULES:
Â  Â  Â  Â  Â  - DO NOT use the exact same words or structure as previous articles.

Â  Â  Â  Â  Â  STRICT OUTPUT FORMAT:
Â  Â  Â  Â  Â  TITLE: [Very Simple SEO-Optimized English Title starting with the main keywords from the original headline]
Â  Â  Â  Â  Â  SLUG: [url-friendly-lowercase-english-slug]
Â  Â  Â  Â  Â  EXCERPT: [Simple SEO-Optimized English Snippet/Hook under 160 characters that includes the main keywords]
Â  Â  Â  Â  Â  CATEGORY: Kenya News
Â  Â  Â  Â  Â  TAGS: [comma, separated, english, seo, tags]
Â  Â  Â  Â  Â  IMAGE_KEYWORD: [search query]
Â  Â  Â  Â  Â  BODY:
Â  Â  Â  Â  Â  [Article text in chatty, simple British English containing the internal link.Â 
Â  Â  Â  Â  Â  Structure it logically:Â 
Â  Â  Â  Â  Â  - Start with an H2 heading that includes the main keywords from the original headline.
Â  Â  Â  Â  Â  - Immediately follow that H2 with a crisp, 40 to 60 word neutral answer paragraph to win Google Featured Snippets.
Â  Â  Â  Â  Â  - Continue with natural subheadings that also reuse the main keywords (AI decides the subheadings). Do not include or repeat any frontmatter content (like title, excerpt) in the BODY. Start the BODY with the first paragraph of the article.]
Â  Â  Â  Â  Â  '''

Â  Â  Â  Â  Â  full_text = ""
Â  Â  Â  Â  Â  success_flag = False
Â  Â  Â  Â  Â  keys_tried = 0
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  for model_id in MODELS_TO_TRY:
Â  Â  Â  Â  Â  Â  Â  # We loop 4 times so it has enough attempts to cycle keys and wait if both are exhausted
Â  Â  Â  Â  Â  Â  Â  for retry in range(4):
Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = client.models.generate_content(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=model_id, contents=prompt,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  config=types.GenerateContentConfig(temperature=0.9, tools=[google_search_tool])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_text = response.text.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success_flag = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âœ… SUCCESS with {model_id}!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  breakÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "429" in str(e):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keys_tried += 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if keys_tried >= len(api_keys):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("â³ All keys hit quota. Waiting 35s...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(35)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keys_tried = 0 # reset tracking after cooldown
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("âš ï¸ Quota hit. Rotating to next API key...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_key = next(key_cycle)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  client = genai.Client(api_key=current_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Error with {model_id}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  Â  if success_flag: break

Â  Â  Â  Â  Â  if not success_flag: sys.exit(1)

Â  Â  Â  Â  Â  print("ğŸ” Gemini raw output preview (first 900 chars):")
Â  Â  Â  Â  Â  print(full_text[:900])

Â  Â  Â  Â  Â  # Strip outer markdown wrappers safely
Â  Â  Â  Â  Â  full_text = re.sub(r'^```(?:markdown|text|html)?\n', '', full_text, flags=re.IGNORECASE)
Â  Â  Â  Â  Â  full_text = re.sub(r'\n```$', '', full_text).strip()

Â  Â  Â  Â  Â  parsed = {"TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": ""}

Â  Â  Â  Â  Â  section_matches = re.findall(r'^(TITLE|SLUG|EXCERPT|CATEGORY|TAGS|IMAGE_KEYWORD):\s*(.*)', full_text, re.MULTILINE | re.IGNORECASE)
Â  Â  Â  Â  Â  for key, value in section_matches:
Â  Â  Â  Â  Â  Â  Â  parsed[key.upper()] = value.strip()

Â  Â  Â  Â  Â  body_match = re.search(r'BODY:\s*(.*)', full_text, re.DOTALL | re.IGNORECASE)
Â  Â  Â  Â  Â  if body_match:
Â  Â  Â  Â  Â  Â  Â  parsed["BODY"] = body_match.group(1).strip()
Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  parsed["BODY"] = full_text.split("BODY:", 1)[-1].strip() if "BODY:" in full_text else full_text

Â  Â  Â  Â  Â  # Scrub out any leaked frontmatter lines from the top of the body
Â  Â  Â  Â  Â  parsed["BODY"] = re.sub(r'^(TITLE|SLUG|EXCERPT|CATEGORY|TAGS|IMAGE_KEYWORD|BODY):\s*.*$', '', parsed["BODY"], flags=re.MULTILINE | re.IGNORECASE).strip()

Â  Â  Â  Â  Â  parsed["BODY"] = dash_scrubber(parsed["BODY"]).strip()

Â  Â  Â  Â  Â  tags_list = [t.strip().replace('"', '') for t in parsed["TAGS"].split(",") if t.strip()]
Â  Â  Â  Â  Â  tags_str = ', '.join([f'"{t}"' for t in tags_list]) if tags_list else '"kenya", "news"'

Â  Â  Â  Â  Â  if not parsed["BODY"] or len(parsed["BODY"]) < 300:
Â  Â  Â  Â  Â  Â  Â  print("âš ï¸ BODY extraction weak - using fallback")
Â  Â  Â  Â  Â  Â  Â  parsed["BODY"] = full_text.split("BODY:", 1)[-1].strip() if "BODY:" in full_text else full_text

Â  Â  Â  Â  Â  final_image_url = target_image if target_image else get_real_image(parsed.get('IMAGE_KEYWORD') or target_title)

Â  Â  Â  Â  Â  final_file = textwrap.dedent(f"""\
Â  Â  Â  Â  Â  ---
Â  Â  Â  Â  Â  title: "{parsed['TITLE'].replace('"', "'")}"
Â  Â  Â  Â  Â  slug: "{parsed['SLUG']}"
Â  Â  Â  Â  Â  excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
Â  Â  Â  Â  Â  image: "{final_image_url}"
Â  Â  Â  Â  Â  category: "{parsed['CATEGORY'] or 'Kenya News'}"
Â  Â  Â  Â  Â  date: "{today_str}"
Â  Â  Â  Â  Â  tags: [{tags_str}]
Â  Â  Â  Â  Â  ---

Â  Â  Â  Â  Â  {parsed['BODY']}
Â  Â  Â  Â  Â  """)

Â  Â  Â  Â  Â  out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
Â  Â  Â  Â  Â  os.makedirs(out_dir, exist_ok=True)
Â  Â  Â  Â  Â  filename = f"{parsed['SLUG']}.md"
Â  Â  Â  Â  Â  with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  Â  f.write(final_file)

Â  Â  Â  Â  Â  memory.append(final_hash)
Â  Â  Â  Â  Â  os.makedirs(os.path.dirname(memory_path), exist_ok=True)
Â  Â  Â  Â  Â  with open(memory_path, 'w') as f: json.dump(memory[-200:], f)

Â  Â  Â  Â  Â  print(f"âœ… Article saved with full body: {filename}")
Â  Â  Â  Â  Â  print(f"Â  Â Body length: {len(parsed['BODY'])} characters")
Â  Â  Â  Â  Â  EOF

Â  Â  Â  - name: Git Safety Pull
Â  Â  Â  Â  run: |
Â  Â  Â  Â  Â  git config --global user.name "github-actions[bot]"
Â  Â  Â  Â  Â  git config --global user.email "github-actions[bot]@users.noreply.github.com"
Â  Â  Â  Â  Â  git add .
Â  Â  Â  Â  Â  git stash -u
Â  Â  Â  Â  Â  git fetch origin main
Â  Â  Â  Â  Â  git pull origin main --rebase
Â  Â  Â  Â  Â  git stash pop || echo "Nothing to pop"

Â  Â  Â  - name: Commit and push article
Â  Â  Â  Â  uses: stefanzweifel/git-auto-commit-action@v5
Â  Â  Â  Â  with:
Â  Â  Â  Â  Â  commit_message: "content: fresh local scoop via direct links ğŸ‡°ğŸ‡ª"
Â  Â  Â  Â  Â  branch: ${{ env.DEFAULT_BRANCH }}
Â  Â  Â  Â  Â  file_pattern: 'content/posts/*.md .github/scrape_memory.json'
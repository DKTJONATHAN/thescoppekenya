name: East Africa Gossip Bot (Google News)

on:
  schedule:
    # Runs every 4 hours
    - cron: '15 4,8,12,16,20 * * *'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Run Reporter Bot
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")

          # BANNED PHRASES (Strict Clean-up)
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "dive in", "tapestry"
          ]

          # --- 2. FETCH GOOGLE NEWS RSS (EAST AFRICA GOSSIP) ---
          # Targeted query for East African entertainment & celebrities
          q = '("Kenya" OR "Uganda" OR "Tanzania" OR "East Africa") (celebrity OR gossip OR entertainment OR "diamond platnumz" OR "eric omondi" OR zari) when:24h'
          safe_q = urllib.parse.quote(q)
          rss_url = f'https://news.google.com/rss/search?q={safe_q}&hl=en-KE&gl=KE&ceid=KE:en'
          
          print(f"üåç Fetching EA Gossip from Google News: {rss_url}")
          
          try:
              resp = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(resp.content, "xml")
              articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")]
          except Exception as e:
              print(f"‚ùå RSS Error: {e}")
              sys.exit(0)

          if not articles:
              print("‚ö†Ô∏è No fresh East African gossip found in RSS.")
              sys.exit(0)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          # --- 3. FREE SCRAPER (STEALTH PLAYWRIGHT) ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
                  context = browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                  )
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

                  try:
                      page.goto(url, timeout=50000)
                      try: page.wait_for_load_state("networkidle", timeout=10000)
                      except: pass

                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      
                      # EXTRACT IMAGE VIA META TAGS
                      img_url = ""
                      og_image = soup.find("meta", property="og:image")
                      if og_image and og_image.get("content"):
                          img_url = og_image["content"]
                      
                      if not img_url:
                          twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
                          if twitter_image and twitter_image.get("content"):
                              img_url = twitter_image["content"]
                      
                      # EXTRACT ARTICLE TEXT
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 600:
                              article_text = chunk
                              break
                      
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                          
                      return article_text, img_url
                  except: return None, None
                  finally: browser.close()

          # --- 4. PROCESS ARTICLES ---
          full_content, final_hash, target_title, target_image = None, None, None, None
          
          for a in articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              if u_hash in memory: continue
              
              result = scrape_article_free(a['url'])
              if not result: continue
              text, img = result
              
              if text and len(text) > 600:
                  full_content = text
                  target_image = img
                  final_hash = u_hash
                  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
                  break 

          if not full_content: 
              print("‚ùå No valid unread gossip found.")
              sys.exit(0)

          print(f"üéØ Target Story: {target_title[:80]}...")

          # --- 5. HELPERS (INTERNAL LINKS & SCRUBBER) ---
          def get_internal_context(posts_dir):
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY IN THE SHENG TEXT USING MARKDOWN):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- Title: {t} | Link: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def get_image(query):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              if not key: return fallback
              try:
                  u = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={key}"
                  r = requests.get(u, timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          def dash_scrubber(text):
              if not text: return ""
              text = text.replace('\u2014', '-').replace('\u2013', '-')
              text = text.replace('‚Äî', '-').replace('‚Äì', '-')
              return text

          # --- 6. GEMINI REWRITE (Reporter Persona) ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())
          
          internal_links_data = get_internal_context(os.environ.get("POSTS_DIR", "content/posts"))

          prompt = f'''Current Date: {full_date_str}
          SOURCE TITLE: "{target_title}"
          SOURCE TEXT: 
          {full_content[:12000]}
          
          {internal_links_data}

          TASK: Write an investigative East African gossip article as a **Nairobi Gen Z Reporter**.
          1. Use Google Search to verify any trending claims in the story if needed.
          2. You MUST naturally insert a markdown link to one of the PAST STORIES provided above somewhere in the body paragraph.

          ROLE & TONE:
          - You are NOT a fan. You are a REPORTER ("The Nairobi Street Insider").
          - Use investigative phrases: "Tumenusa hii story", "Inspecta wetu wameconfirm", "Receipts ziko".
          - Vibe: Professional but Street Smart. Savage but factual.

          LANGUAGE MANDATE (STRICT):
          - WRITE IN PURE KENYAN SHENG ONLY.
          - NO FORMAL ENGLISH. NO OLD SLANG.
          - Find trending terms autonomously.
          - BANNED: {", ".join(BANNED_PHRASES)}.
          - NEVER use em dashes or en dashes. Only use regular hyphens.

          STRICT OUTPUT FORMAT:
          TITLE: [Investigative Sheng Clickbait]
          SLUG: [url-friendly-lowercase]
          EXCERPT: [Reporter hook in Sheng]
          CATEGORY: Entertainment
          TAGS: [comma, separated, tags]
          IMAGE_KEYWORD: [search query based on title]
          BODY:
          [Article text in pure Sheng containing the internal link. You MUST structure the body using EXACTLY these three Markdown headings:]

          ## Scoop Ni Gani?
          [What happened?]

          ## Evidence Ni?
          [The receipts/details/evidence]

          ## Verdict Yetu Ni
          [Your reporter opinion/conclusion]
          '''

          print("ü§ñ Generating with Gemini...")
          final_text = ""
          success_flag = False
          
          for model in ["gemini-3-flash-preview", "gemini-2.5-flash"]:
              try:
                  resp = client.models.generate_content(
                      model=model,
                      contents=prompt,
                      config=types.GenerateContentConfig(
                          temperature=0.9,
                          tools=[google_search_tool]
                      )
                  )
                  final_text = dash_scrubber(resp.text.strip())
                  success_flag = True
                  print(f"‚úÖ Success with {model}")
                  break
              except Exception as e:
                  if "429" in str(e):
                      print("‚è≥ Quota hit. Waiting 35s...")
                      time.sleep(35)
                      continue
                  print(f"‚ö†Ô∏è Error with {model}: {e}")
                  break

          if not success_flag or not final_text:
              sys.exit(1)

          # --- 7. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          
          for line in final_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("```"): continue # Prevent stray markdown fences from polluting body
              
              if clean.startswith("TITLE:"): parsed["TITLE"] = dash_scrubber(clean.replace("TITLE:", "").strip())
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = dash_scrubber(clean.replace("EXCERPT:", "").strip())
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  # Check if Gemini sneaks the start of the body on the same line
                  inline_body = line.split("BODY:", 1)[-1].strip()
                  if inline_body: parsed["BODY"] += inline_body + "\n"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          # Use scraped image, fallback to unsplash if scraped image is missing
          final_image_url = target_image if target_image else get_image(parsed['IMAGE_KEYWORD'] or target_title)

          # Use textwrap.dedent to ensure the final markdown file is completely flush-left
          md_content = textwrap.dedent(f"""\
          ---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{final_image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """)

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          filename = f"{parsed['SLUG']}.md"
          
          with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
              f.write(md_content)
              
          # Update Memory
          memory.append(final_hash)
          os.makedirs(os.path.dirname(memory_path), exist_ok=True)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          
          print(f"‚úÖ Published: {filename}")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh east african gossip via Google News üïµÔ∏è"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'content/posts/*.md .github/scrape_memory.json'
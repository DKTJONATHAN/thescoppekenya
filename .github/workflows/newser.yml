name: Standard Media Gossip (Sheng Reporter)

on:
  schedule:
    # Runs every 4 hours
    - cron: '15 4,8,12,16,20 * * *'
  workflow_dispatch:

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4 lxml

      - name: Run Reporter Bot
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, sys
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup

          # --- 1. CONFIGURATION ---
          RSS_URL = "https://www.standardmedia.co.ke/rss/entertainment.php"
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          
          # SAFETY FILTER: Skip these boring pages if they appear in RSS
          BORING_KEYWORDS = ["Policy", "Terms", "Conditions", "Contact Us", "About Us", "Sitemap", "Advertise", "Subscribe"]

          # BANNED PHRASES (Strict Clean-up)
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "dive in", "tapestry"
          ]

          # --- 2. FETCH RSS FEED ---
          print(f"ðŸ“¡ Scanning Standard Media RSS: {RSS_URL}")
          try:
              rss_resp = requests.get(RSS_URL, timeout=15)
              soup = BeautifulSoup(rss_resp.content, "xml")
              all_items = soup.find_all("item")
          except Exception as e:
              print(f"âŒ RSS Error: {e}")
              sys.exit(0)

          if not all_items:
              print("âš ï¸ No items found in RSS.")
              sys.exit(0)

          # --- 3. FILTER & SELECT ---
          valid_items = []
          for item in all_items:
              title = item.title.text.strip()
              link = item.link.text.strip()
              
              # Check if title contains any boring keywords
              if any(b in title for b in BORING_KEYWORDS):
                  print(f"ðŸš« Skipping boring page: {title}")
                  continue
                  
              valid_items.append(item)

          if not valid_items:
              print("âŒ No valid news stories found (only boring pages).")
              sys.exit(0)

          # Pick one random story from the valid list (Top 8 to ensure variety)
          target_item = random.choice(valid_items[:8])
          target_link = target_item.link.text.strip()
          target_title = target_item.title.text.strip()
          
          print(f"ðŸŽ¯ Selected Story: {target_title}")
          print(f"ðŸ”— Link: {target_link}")

          # --- 4. EXTRACT FULL CONTENT VIA APIFY ---
          print("ðŸ•·ï¸ Extracting full content via Apify...")
          full_content = ""
          
          try:
              apify_client = ApifyClient(os.environ['APIFY_TOKEN'])
              run = apify_client.actor("apify/website-content-crawler").call(
                  run_input={
                      "startUrls": [{"url": target_link}], 
                      "maxCrawlPages": 1,
                      "crawlerType": "playwright:firefox",
                      "initialCookies": [] 
                  }
              )
              
              dataset = apify_client.dataset(run["defaultDatasetId"])
              items = list(dataset.iterate_items())
              
              if items:
                  full_content = items[0].get('markdown', items[0].get('text', ''))
              
          except Exception as e:
              print(f"âŒ Apify Extraction Failed: {e}")
              full_content = target_item.description.text.strip()

          if len(full_content) < 200:
              print("âš ï¸ Content too short. Aborting.")
              sys.exit(0)

          # --- 5. GEMINI REWRITE (Reporter Persona) ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())

          prompt = f'''
          SOURCE TITLE: "{target_title}"
          SOURCE TEXT: 
          {full_content[:12000]}

          TASK: Write an investigative gossip article as a **Nairobi Gen Z Reporter**.

          ROLE & TONE:
          - You are NOT a fan. You are a REPORTER ("The Nairobi Street Insider").
          - Use investigative phrases: "Tumenusa hii story" (We sniffed this out), "Inspecta wetu wameconfirm" (Our inspectors confirmed), "Receipts ziko" (We have receipts).
          - Vibe: Professional but Street Smart. Savage but factual.

          LANGUAGE MANDATE (STRICT):
          - WRITE IN PURE KENYAN SHENG ONLY.
          - NO FORMAL ENGLISH. NO OLD SLANG.
          - Find trending terms autonomously.
          - BANNED: {", ".join(BANNED_PHRASES)}.

          STRUCTURE:
          1. THE SCOOP (What happened?)
          2. THE RECEIPTS (The details/evidence)
          3. THE VERDICT (Your reporter opinion)

          OUTPUT FORMAT:
          TITLE: [Investigative Sheng Clickbait]
          SLUG: [url-friendly-lowercase]
          EXCERPT: [Reporter hook in Sheng]
          CATEGORY: Entertainment
          TAGS: [comma, separated, tags]
          IMAGE_KEYWORD: [search query based on title]
          BODY:
          [Full Article in Markdown]
          '''

          print("ðŸ¤– Generating with Gemini...")
          final_text = ""
          
          for model in ["gemini-3-flash-preview", "gemini-2.5-flash"]:
              try:
                  resp = client.models.generate_content(
                      model=model,
                      contents=prompt,
                      config=types.GenerateContentConfig(
                          temperature=0.9,
                          tools=[google_search_tool]
                      )
                  )
                  final_text = resp.text.strip().replace('\u2014', '-').replace('\u2013', '-')
                  print(f"âœ… Success with {model}")
                  break
              except Exception as e:
                  print(f"âš ï¸ Error with {model}: {e}")
                  time.sleep(2)
                  continue

          if not final_text:
              sys.exit(1)

          # --- 6. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          
          for line in final_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          def get_image(query):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              if not key: return fallback
              try:
                  u = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={key}"
                  r = requests.get(u, timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          image_url = get_image(parsed['IMAGE_KEYWORD'] or target_title)

          md_content = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          filename = f"{parsed['SLUG']}.md"
          
          with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
              f.write(md_content)
          
          print(f"âœ… Published: {filename}")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: standard media scoop ðŸ•µï¸"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
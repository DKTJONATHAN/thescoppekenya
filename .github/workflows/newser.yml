name: African Entertainment British Gossip Bot (Google News)

on:
  schedule:
    - cron: '15 4,8,12,16,20 * * *'
  workflow_dispatch:

concurrency:
  group: \( {{ github.workflow }}- \){{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Run Reporter Bot
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")

          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "dive in", "tapestry", "mambo vipi", "niaje wasee", "karibuni",
              "delve into", "moreover", "furthermore", "in conclusion", 
              "it's worth noting", "a testament to"
          ]

          OPENING_VIBES = [
              "Start with a cheeky rhetorical question asking the reader if they are ready for this mess.",
              "Start by pretending to whisper a secret you just found out about these celebrities.",
              "Start with a blunt, slightly sarcastic summary of the drama.",
              "Start by acting exhausted by the constant showbiz drama but admitting you absolutely love it.",
              "Start by expressing shock at how cheeky these celebs are being online today.",
              "Start with a bold, controversial statement that challenges a popular opinion about the artist/actor.",
              "Start by triggering FOMO (Fear Of Missing Out), making the reader feel they are the last to know this crucial gossip."
          ]
          random_hook = random.choice(OPENING_VIBES)

          # --- 2. FETCH GOOGLE NEWS RSS (PAN-AFRICAN ENTERTAINMENT) ---
          q = '("Nigeria" OR "South Africa" OR "Ghana" OR "Kenya" OR "Tanzania" OR "Africa") (afrobeats OR amapiano OR nollywood OR "social media trend" OR "music video" OR celebrity OR showbiz) when:2d'
          safe_q = urllib.parse.quote(q)
          rss_url = f'https://news.google.com/rss/search?q={safe_q}&hl=en-KE&gl=KE&ceid=KE:en'
          
          print(f"üåç Fetching African Entertainment News: {rss_url}")
          
          try:
              resp = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(resp.content, "xml")
              articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")]
          except Exception as e:
              print(f"‚ùå RSS Error: {e}")
              sys.exit(0)

          if not articles:
              print("‚ö†Ô∏è No fresh African entertainment gossip found in RSS.")
              sys.exit(0)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          # --- 3. FREE SCRAPER (unchanged) ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading Google News Link: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox", "--disable-dev-shm-usage"])
                  context = browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                      viewport={"width": 1920, "height": 1080}
                  )
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                  try:
                      page.goto(url, timeout=60000)
                      try:
                          page.wait_for_function("window.location.hostname && !window.location.hostname.includes('google.com')", timeout=20000)
                      except: pass
                      if "google.com" in page.url:
                          try:
                              hrefs = page.evaluate("Array.from(document.querySelectorAll('a')).map(a => a.href).filter(h => h.startsWith('http') && !h.includes('google.com'))")
                              if hrefs:
                                  print(f"Found hidden direct link: {hrefs[0]}")
                                  page.goto(hrefs[0], timeout=60000)
                          except: pass
                      page.wait_for_timeout(5000)
                      print(f"üìç Successfully reached full article: {page.url}")
                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      img_url = ""
                      og_image = soup.find("meta", property="og:image")
                      if og_image and og_image.get("content"): img_url = og_image["content"]
                      if not img_url:
                          twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
                          if twitter_image and twitter_image.get("content"): img_url = twitter_image["content"]
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 300:
                              article_text = chunk
                              break
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                      return article_text, img_url
                  except Exception as e: 
                      print(f"Scrape error: {e}")
                      return None, None
                  finally: browser.close()

          # --- 4. PROCESS ARTICLES ---
          full_content, final_hash, target_title, target_image = None, None, None, None
          for a in articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              if u_hash in memory: continue
              result = scrape_article_free(a['url'])
              if not result: continue
              text, img = result
              if text and len(text) > 300:
                  full_content = text
                  target_image = img
                  final_hash = u_hash
                  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
                  break 

          if not full_content: 
              print("‚ùå No valid unread gossip found.")
              sys.exit(0)

          print(f"üéØ Target Story: {target_title[:80]}...")

          # --- 5. HELPERS ---
          def get_internal_context(posts_dir):
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY IN THE TEXT USING MARKDOWN):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- Title: {t} | Link: https://zandani.co.ke/posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def get_image(query):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              if not key: return fallback
              try:
                  u = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={key}"
                  r = requests.get(u, timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          def dash_scrubber(text):
              if not text: return ""
              text = text.replace('\u2014', '-').replace('\u2013', '-')
              text = text.replace(chr(8212), '-').replace(chr(8211), '-')
              return text

          # --- 6. GEMINI REWRITE ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())
          internal_links_data = get_internal_context(os.environ.get("POSTS_DIR", "content/posts"))

          prompt = f'''Current Date: {full_date_str}
          SOURCE TITLE: "{target_title}"
          SOURCE TEXT: 
          {full_content[:12000]}
          
          {internal_links_data}

          TASK: Write an investigative Pan-African entertainment gossip article.
          1. Use Google Search to verify any trending claims in the story if needed.
          2. You MUST naturally insert a markdown link to one of the PAST STORIES provided above somewhere in the body paragraph.

          LANGUAGE MANDATE (STRICT):
          - NO SHENG OR SWAHILI. You are writing for an audience who loves simple, dramatic British gossip.
          - Use very simple, conversational British English. Think of a chatty, cheeky tabloid reporter having a chinwag about Afrobeats, Amapiano, or Nollywood stars.
          - Keep the vocabulary basic but the tone highly dramatic and gossipy. 
          - Use conversational fillers naturally to sound human: "Right,", "Well,", "Honestly,", "Anyway,", "Mate."
          - Include rhetorical questions and subjective asides in brackets (e.g., "Can you even imagine?", "What a nightmare.").
          - Vary sentence length aggressively. Mix short, punchy facts with longer, breathy gossip thoughts.
          - BANNED PHRASES: {", ".join(BANNED_PHRASES)}.
          - NEVER use long dashes or double dashes. Only use regular single hyphens.

          ANTI-REPETITION RULES:
          - DO NOT use the exact same words or structure as previous articles. 
          - HERE IS YOUR UNIQUE OPENING INSTRUCTION FOR THIS SPECIFIC ARTICLE: {random_hook}

          EXAMPLE OF THE PERFECT TONE VIBE TO MIMIC (DO NOT COPY THESE EXACT WORDS):
          "Right, gather round. If you thought yesterday was dramatic in the Afrobeats scene, you are in for an absolute treat. The internet is having a right laugh about this one. The receipts say it all..."

          STRICT OUTPUT FORMAT:
          TITLE: [Very Simple SEO-Optimized English Title front-loaded with the main keyword]
          SLUG: [url-friendly-lowercase-english-slug]
          EXCERPT: [Simple SEO-Optimized English Snippet/Hook under 160 characters]
          CATEGORY: Entertainment
          TAGS: [comma, separated, english, seo, tags]
          IMAGE_KEYWORD: [search query based on title]
          BODY:
          [Article text in chatty British English containing the internal link. You MUST structure the body using EXACTLY these Markdown headings:]

          ## [Direct Question About the Core Gossip]
          [40 to 60-word completely objective answer paragraph to win Google Featured Snippets]

          ## The Scoop
          [What actually happened in the drama?]

          ## The Evidence
          [The receipts/details/proof from social media or statements]

          ## The Verdict
          [Your cheeky British reporter opinion/conclusion]
          '''

          print("ü§ñ Generating with Gemini...")
          final_text = ""
          success_flag = False
          
          for model in ["gemini-3-flash-preview", "gemini-2.5-flash"]:
              for retry in range(2):
                  try:
                      resp = client.models.generate_content(
                          model=model,
                          contents=prompt,
                          config=types.GenerateContentConfig(temperature=0.9, tools=[google_search_tool])
                      )
                      final_text = dash_scrubber(resp.text.strip())
                      success_flag = True
                      print(f"‚úÖ Success with {model}")
                      break
                  except Exception as e:
                      if "429" in str(e):
                          print("‚è≥ Quota hit. Waiting 35s...")
                          time.sleep(35)
                          continue
                      print(f"‚ö†Ô∏è Error with {model}: {e}")
                      break
              if success_flag: break

          if not success_flag or not final_text:
              sys.exit(1)

          # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          # 7. ROBUST PARSE (THE FIX)
          # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          print("üîç Gemini raw output preview (first 900 chars):")
          print(final_text[:900])

          parsed = {"TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": ""}

          # Extract headers
          section_matches = re.findall(r'^(TITLE|SLUG|EXCERPT|CATEGORY|TAGS|IMAGE_KEYWORD|BODY):\s*(.*)', final_text, re.MULTILINE | re.IGNORECASE)
          for key, value in section_matches:
              key = key.upper()
              if key != "BODY":
                  parsed[key] = value.strip()

          # Extract full BODY
          body_match = re.search(r'BODY:\s*(.*)', final_text, re.DOTALL | re.IGNORECASE)
          if body_match:
              parsed["BODY"] = body_match.group(1).strip()

          # Cleanups
          parsed["BODY"] = re.sub(r'```[\s\S]*?```', '', parsed["BODY"])
          parsed["BODY"] = dash_scrubber(parsed["BODY"]).strip()

          # Proper tags
          tags_list = [t.strip().replace('"', '') for t in parsed["TAGS"].split(",") if t.strip()]
          tags_str = ', '.join([f'"{t}"' for t in tags_list]) if tags_list else '"afrobeats", "nollywood", "gossip"'

          # Strong fallback
          if not parsed["BODY"] or len(parsed["BODY"]) < 300:
              print("‚ö†Ô∏è BODY extraction weak - using fallback")
              parsed["BODY"] = final_text.split("BODY:", 1)[-1].strip() if "BODY:" in final_text else final_text

          final_image_url = target_image if target_image else get_image(parsed.get('IMAGE_KEYWORD') or target_title)

          md_content = textwrap.dedent(f"""\
          ---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{final_image_url}"
          category: "{parsed['CATEGORY'] or 'Entertainment'}"
          date: "{today_str}"
          tags: [{tags_str}]
          ---

          {parsed['BODY']}
          """)

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          filename = f"{parsed['SLUG']}.md"
          
          with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
              f.write(md_content)
              
          memory.append(final_hash)
          os.makedirs(os.path.dirname(memory_path), exist_ok=True)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          
          print(f"‚úÖ Published: {filename}")
          print(f"   Body length: {len(parsed['BODY'])} characters")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh african entertainment gossip via direct links üïµÔ∏è"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'content/posts/*.md .github/scrape_memory.json'
name: Global Sports & Tech British Gossip Scraper (Google News + Internal Links)

on:
  schedule:
    # 5 runs distributed from 6:00 AM EAT to 10:00 PM EAT
    - cron: '0 3 * * *'   # 6:00 AM EAT (3:00 AM UTC)
    - cron: '0 7 * * *'   # 10:00 AM EAT (7:00 AM UTC)
    - cron: '0 11 * * *'  # 2:00 PM EAT (11:00 AM UTC)
    - cron: '0 15 * * *'  # 6:00 PM EAT (3:00 PM UTC)
    - cron: '0 19 * * *'  # 10:00 PM EAT (7:00 PM UTC)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic: european_football, other_sports, tech'
        required: false
        default: ''

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          MODELS_TO_TRY = ["gemini-3-flash-preview", "gemini-2.5-flash"]
          
          # Humanizer: added sports/tech AI cliches to banned list
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "mambo vipi", "niaje wasee", "karibuni", "tapestry", "dive in",
              "a testament to their skill", "delve into the stats", 
              "navigating the digital landscape", "in today's fast-paced world",
              "moreover", "furthermore", "in conclusion"
          ]

          # DYNAMIC OPENINGS TO PREVENT REPETITION (Sports/Tech Psychological Hooks)
          OPENING_VIBES = [
              "Start with a bold, controversial statement that challenges a popular opinion about a manager, player, or tech brand.",
              "Start by making the reader feel they are the absolute last to know about a massive gadget leak or a shocking transfer rumour.",
              "Start by expressing absolute disbelief at a ridiculous scoreline, controversial referee decision, or the shocking price tag of a new device.",
              "Start with a funny, relatable anecdote about battery life dying at the worst time or watching your team bottle a lead.",
              "Start directly with a savage, sarcastic comment about the losing side or an outdated piece of tech."
          ]
          random_hook = random.choice(OPENING_VIBES)

          hour_to_topic = {
              3: 'european_football', 7: 'tech', 11: 'european_football',
              15: 'other_sports', 19: 'tech'
          }

          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          valid_topics = ['european_football', 'other_sports', 'tech']
          if manual_input in valid_topics: topic = manual_input
          else: topic = hour_to_topic.get(current_hour, 'european_football')
          print(f"üéØ Topic: {topic}")

          # --- 2. FETCH GOOGLE NEWS RSS ---
          base_keywords = {
              'european_football': 'premier league OR arsenal OR manchester united OR chelsea OR la liga OR real madrid OR champions league OR bundesliga OR serie a',
              'other_sports': 'nba OR formula 1 OR wrc safari rally OR tennis grand slam OR rugby sevens',
              'tech': 'smartphone review OR new phone release OR tech gadget OR samsung galaxy OR iphone OR device alerts'
          }
          query_str = base_keywords.get(topic, 'sports news')
          safe_query = urllib.parse.quote(f"{query_str} when:24h")
          
          rss_url = f"https://news.google.com/rss/search?q={safe_query}&hl=en-KE&gl=KE&ceid=KE:en"
          print(f"üåç Fetching Google News: {rss_url}")
          
          try:
              resp = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(resp.content, "xml")
              articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")]
          except Exception as e:
              print(f"Error fetching Google News: {e}")
              sys.exit(0)

          if not articles:
              print("No fresh news, skipping")
              sys.exit(0)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          # --- 3. FREE SCRAPER ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
                  context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36")
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                  try:
                      page.goto(url, timeout=50000)
                      try: page.wait_for_load_state("networkidle", timeout=10000)
                      except: pass
                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      
                      img_url = ""
                      og_image = soup.find("meta", property="og:image")
                      if og_image and og_image.get("content"): img_url = og_image["content"]
                      if not img_url:
                          twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
                          if twitter_image and twitter_image.get("content"): img_url = twitter_image["content"]
                      
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 600:
                              article_text = chunk
                              break
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                      return article_text, img_url
                  except: return None, None
                  finally: browser.close()

          # --- 4. PROCESS ARTICLES ---
          full_raw_text, final_hash, target_title, target_image = None, None, None, None
          for a in articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              if u_hash in memory: continue
              result = scrape_article_free(a['url'])
              if not result: continue
              text, img = result
              if text and len(text) > 600:
                  full_raw_text = text
                  target_image = img
                  final_hash = u_hash
                  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
                  break 

          if not full_raw_text: 
              print("‚ùå No valid unread content found.")
              sys.exit(0)

          # --- 5. HELPERS ---
          def get_internal_context(posts_dir):
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY IN THE TEXT USING MARKDOWN):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- Title: {t} | Link: https://zandani.co.ke/posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not access_key: return fallback
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200: return resp.json()['urls']['regular']
              except: pass
              return fallback

          def dash_scrubber(text):
              if not text: return ""
              text = text.replace('\u2014', '-').replace('\u2013', '-')
              text = text.replace(chr(8212), '-').replace(chr(8211), '-')
              return text

          # --- 6. GENERATION ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())
          internal_links_data = get_internal_context(os.environ.get("POSTS_DIR", "content/posts"))

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{target_title}"
          SOURCE SCRAPED TEXT: "{full_raw_text[:12000]}"
          
          {internal_links_data}

          TASK: Write an article as a **British Sports & Tech Pundit**.
          1. Use Google Search to research the latest details on this story if needed.
          2. Naturally insert a markdown link to one of the PAST STORIES provided above somewhere in the body paragraph.

          LANGUAGE MANDATE (STRICT):
          - NO SHENG OR SWAHILI. Write for an audience who loves cheeky British gossip, sports banter, and tech leaks.
          - Use conversational British English: "Right," "Honestly," "Mate," "Look," "absolute madness."
          - Include rhetorical questions and bracketed side thoughts (e.g., "and who actually pays that much?").
          - BANNED PHRASES: {", ".join(BANNED_PHRASES)}.
          - NEVER use long dashes or double dashes. Only use regular hyphens.

          ANTI-REPETITION RULES:
          - DO NOT use the exact same words or structure as previous articles. 
          - HERE IS YOUR UNIQUE OPENING INSTRUCTION FOR THIS SPECIFIC ARTICLE: {random_hook}

          EXAMPLE OF THE PERFECT TONE VIBE TO MIMIC (DO NOT COPY THESE EXACT WORDS):
          "Look, if you actually thought they were going to hold on to that lead, you are having a laugh. The tactics were completely all over the place..." 
          (OR FOR TECH): "Right, get a load of this. The new specs just dropped, and frankly, I am entirely unconvinced. For that price tag?"

          STRICT OUTPUT FORMAT:
          TITLE: [Very Simple SEO-Optimized English Title front-loaded with the main keyword]
          SLUG: [url-friendly-lowercase-english-slug]
          EXCERPT: [Simple SEO-Optimized English Snippet/Hook under 160 characters]
          CATEGORY: Global News
          TAGS: [comma, separated, english, seo, tags]
          IMAGE_KEYWORD: [search query]
          BODY:
          [Article text in chatty British English containing the internal link. 
          Structure it logically:
          - Include an H2 heading asking a direct question about the core match/tech news.
          - Immediately follow that H2 with a crisp, completely objective 40 to 60-word answer paragraph to win Google Featured Snippets.
          - Then dive into the rest of the cheeky British banter: The Core Update -> The Deeper Analysis -> The Verdict.]
          '''

          full_text = ""
          success_flag = False
          
          for model_id in MODELS_TO_TRY:
              for retry in range(2):
                  try:
                      response = client.models.generate_content(
                          model=model_id, contents=prompt,
                          config=types.GenerateContentConfig(temperature=0.9, tools=[google_search_tool])
                      )
                      full_text = dash_scrubber(response.text.strip())
                      success_flag = True
                      print(f"‚úÖ SUCCESS with {model_id}!")
                      break 
                  except Exception as e:
                      if "429" in str(e):
                          print("‚è≥ Quota hit. Waiting 35s...")
                          time.sleep(35)
                          continue
                      break
              if success_flag: break

          if not success_flag: sys.exit(1)

          # --- 7. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("```"): continue
              if clean.startswith("TITLE:"): parsed["TITLE"] = dash_scrubber(clean.replace("TITLE:", "").strip())
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = dash_scrubber(clean.replace("EXCERPT:", "").strip())
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  inline_body = line.split("BODY:", 1)[-1].strip()
                  if inline_body: parsed["BODY"] += inline_body + "\n"
                  continue
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          final_image_url = target_image if target_image else get_real_image(parsed['IMAGE_KEYWORD'] or target_title)
          
          final_file = textwrap.dedent(f"""\
          ---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{final_image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """)
          
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{parsed['SLUG']}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          
          memory.append(final_hash)
          os.makedirs(os.path.dirname(memory_path), exist_ok=True)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"‚úÖ File saved: {parsed['SLUG']}.md")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh sports & tech scoop via Google News üåç"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'content/posts/*.md .github/scrape_memory.json'
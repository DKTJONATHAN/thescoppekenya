name: Global Sports & Tech British Gossip Scraper (Google News + Internal Links)

on:
  schedule:
    - cron: '0 3 * * *'   # 6:00 AM EAT
    - cron: '0 7 * * *'   # 10:00 AM EAT
    - cron: '0 11 * * *'  # 2:00 PM EAT
    - cron: '0 15 * * *'  # 6:00 PM EAT
    - cron: '0 19 * * *'  # 10:00 PM EAT
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic: sports, tech'
        required: false
        default: ''

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap, itertools
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright

          # 1. CONFIGURATION 
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          MODELS_TO_TRY = ["gemini-3-flash-preview", "gemini-2.5-flash"]
          
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "mambo vipi", "niaje wasee", "karibuni", "tapestry", "dive in",
              "a testament to their skill", "delve into the stats", 
              "navigating the digital landscape", "in today's fast-paced world",
              "moreover", "furthermore", "in conclusion"
          ]

          hour_to_topic = {
              3: 'sports', 7: 'tech', 11: 'sports',
              15: 'sports', 19: 'tech'
          }

          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          valid_topics = ['sports', 'tech']
          if manual_input in valid_topics:
              topic = manual_input
          else:
              topic = hour_to_topic.get(current_hour, 'sports')
          print(f"üéØ Topic: {topic}")

          # 2. DYNAMIC GOOGLE NEWS RSS (Country Codes for Sports) 
          base_keywords = {
              'sports': 'sports OR athletics OR football OR rugby OR basketball',
              'tech': 'smartphone review OR new phone release OR tech gadget OR samsung galaxy OR iphone OR device alerts'
          }
          query_str = base_keywords.get(topic, 'sports news')
          safe_query = urllib.parse.quote(f"{query_str} when:1d") 
          
          if topic == 'sports':
              # Pick a random country code to get global sports directly
              country_code = random.choice(['GB', 'US', 'KE', 'ZA', 'AU', 'NG', 'IN'])
              lang = 'en-GB' if country_code in ['GB', 'ZA', 'AU', 'NG', 'IN', 'KE'] else 'en-US'
              ceid = f"{country_code}:en"
              gl = country_code
          else:
              gl = 'KE'
              lang = 'en-KE'
              ceid = 'KE:en'

          # Constructed securely to prevent connection adapter formatting errors
          rss_url = f"https://news.google.com/rss/search?q={safe_query}&hl={lang}&gl={gl}&ceid={ceid}"
          print(f"üåç Fetching Google News (Country: {gl} | last 24h): {rss_url}")
          
          try:
              resp = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(resp.content, "xml")
              articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")][:8] 
          except Exception as e:
              print(f"Error fetching Google News: {e}")
              sys.exit(0)

          if not articles:
              print("‚ùå NO ARTICLES FOUND IN LAST 24 HOURS. Skipping generation.")
              sys.exit(0)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          memory_set = set(memory)

          fresh_articles = []
          for a in articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              if u_hash not in memory_set:
                  fresh_articles.append(a)

          if not fresh_articles:
              print("‚ùå NO NEW UNREAD ARTICLES IN LAST 24 HOURS. All recent ones already processed. Skipping.")
              sys.exit(0)

          print(f"Found {len(fresh_articles)} fresh unread articles from last 24h")

          # 3. FAST & RELIABLE SCRAPER 
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Scraping: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
                  context = browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                      viewport={"width": 1920, "height": 1080}
                  )
                  page = context.new_page()
                  try:
                      page.goto(url, timeout=45000, wait_until="domcontentloaded")
                      if "google.com" in page.url or "news.google.com" in page.url:
                          print("   Redirecting from Google...")
                          try:
                              page.wait_for_load_state("networkidle", timeout=12000)
                          except:
                              pass
                          direct = page.evaluate('''() => {
                              let links = Array.from(document.querySelectorAll('a[href^="http"]'));
                              for (let a of links) {
                                  let h = a.href;
                                  if (!h.includes('google') && h.length > 40) return h;
                              }
                              return null;
                          }''')
                          if direct:
                              page.goto(direct, timeout=45000, wait_until="networkidle")
                          else:
                              try:
                                  page.click('a:has-text("Visit")', timeout=8000)
                              except:
                                  pass

                      page.wait_for_timeout(4000)
                      page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                      page.wait_for_timeout(3000)

                      print(f"   Reached: {page.url}")
                      soup = BeautifulSoup(page.content(), "html.parser")

                      article_text = ""
                      main = soup.select_one('article, main, .article-body, .post-content, .entry-content, .content')
                      if main:
                          paras = main.find_all('p')
                          article_text = "\n\n".join(p.get_text(strip=True) for p in paras if len(p.get_text(strip=True)) > 40)
                      
                      if len(article_text) < 500:
                          all_p = soup.find_all('p')
                          article_text = "\n\n".join(p.get_text(strip=True) for p in all_p if len(p.get_text(strip=True)) > 40)

                      img_url = ""
                      og = soup.find("meta", property="og:image")
                      if og and og.get("content"):
                          img_url = og["content"]

                      print(f"   Extracted {len(article_text)} chars")
                      return article_text, img_url
                  except Exception as e:
                      print(f"   Failed: {str(e)[:80]}...")
                      return None, None
                  finally:
                      browser.close()

          # 4. PROCESS ARTICLES 
          full_raw_text, final_hash, target_title, target_image = None, None, None, None
          for a in fresh_articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              text, img = scrape_article_free(a['url'])
              if text and len(text) > 800:
                  full_raw_text = text
                  target_image = img
                  final_hash = u_hash
                  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
                  break
              time.sleep(0.8) 

          if not full_raw_text:
              print("‚ùå NO USABLE FULL ARTICLE FOUND IN LAST 24 HOURS (all short or failed). Skipping generation.")
              sys.exit(0)

          # 5. HELPERS 
          def get_internal_context(posts_dir):
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
                          txt = c.read()
                          t = re.search(r'title:\s*"(.*?)"', txt)
                          t = t.group(1) if t else f
                          context += f"- {t} -> https://zandani.co.ke/posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def get_real_image(query):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not key: return fallback
              try:
                  r = requests.get(f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={key}", timeout=8)
                  if r.status_code == 200: return r.json()['urls']['regular']
              except: pass
              return fallback

          def dash_scrubber(text):
              if not text: return ""
              text = text.replace('\u2014', '-').replace('\u2013', '-')
              text = text.replace(chr(8212), '-').replace(chr(8211), '-')
              return text

          # --- API KEY ROTATION LOGIC ---
          raw_keys = [os.environ.get("GEMINI_WRITE_KEY"), os.environ.get("GEMINI_API_KEY")]
          api_keys = [k for k in raw_keys if k and k.strip()]
          
          if not api_keys:
              print("‚ùå No Gemini API keys provided in environment.")
              sys.exit(1)

          key_cycle = itertools.cycle(api_keys)
          current_key = next(key_cycle)
          client = genai.Client(api_key=current_key)

          google_search_tool = types.Tool(google_search=types.GoogleSearch())
          internal_links_data = get_internal_context(os.environ.get("POSTS_DIR"))

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{target_title}"
          SOURCE TEXT: "{full_raw_text[:10000]}"
          
          {internal_links_data}

          TASK: Write an article as a **British Sports & Tech Pundit**.
          1. Use Google Search if needed for latest details.
          2. Insert one natural markdown link to a PAST STORY in the body.

          LANGUAGE: Conversational British English, cheeky gossip tone. Use "Right,", "Mate,", "absolute madness", rhetorical questions, bracketed asides.
          BANNED: {", ".join(BANNED_PHRASES)}.

          OUTPUT FORMAT (exact):
          TITLE: [SEO title with main keyword first]
          SLUG: [kebab-case-slug]
          EXCERPT: [hook <160 chars]
          CATEGORY: Global News
          TAGS: [comma separated seo tags]
          IMAGE_KEYWORD: [unsplash query]
          BODY:
          [Start with H2 using main keyword.
          Follow with 40 to 60 word neutral answer paragraph.
          Then natural subheadings (AI chooses). No frontmatter repeat in body.]'''

          full_text = ""
          success_flag = False
          keys_tried = 0
          
          for model_id in MODELS_TO_TRY:
              # Loop 4 times to allow switching keys and waiting
              for retry in range(4):
                  try:
                      response = client.models.generate_content(
                          model=model_id, contents=prompt,
                          config=types.GenerateContentConfig(temperature=0.85, tools=[google_search_tool])
                      )
                      full_text = response.text.strip()
                      success_flag = True
                      print(f"‚úÖ Gemini success with {model_id}")
                      break
                  except Exception as e:
                      if "429" in str(e):
                          keys_tried += 1
                          if keys_tried >= len(api_keys):
                              print("‚è≥ All keys hit quota. Waiting 40s...")
                              time.sleep(40)
                              keys_tried = 0 # reset tracking after cooldown
                          else:
                              print("‚ö†Ô∏è Quota hit. Rotating to next API key...")
                              current_key = next(key_cycle)
                              client = genai.Client(api_key=current_key)
                          continue
                      
                      print(f"Gemini error: {e}")
                      break
              if success_flag: break

          if not success_flag: sys.exit(1)

          # 7. PARSE & SAVE 
          print("üîç Gemini output preview:", full_text[:900])

          # Strip outer markdown wrappers safely
          full_text = re.sub(r'^```(?:markdown|text|html)?\n', '', full_text, flags=re.IGNORECASE)
          full_text = re.sub(r'\n```$', '', full_text).strip()

          parsed = {"TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": ""}

          section_matches = re.findall(r'^(TITLE|SLUG|EXCERPT|CATEGORY|TAGS|IMAGE_KEYWORD|BODY):\s*(.*)', full_text, re.MULTILINE | re.IGNORECASE)
          for key, value in section_matches:
              key = key.upper()
              if key != "BODY":
                  parsed[key] = value.strip()

          # Isolate the body content
          body_match = re.search(r'BODY:\s*(.*)', full_text, re.DOTALL | re.IGNORECASE)
          if body_match:
              parsed["BODY"] = body_match.group(1).strip()
          else:
              # Fallback if BODY tag is totally missing
              parsed["BODY"] = full_text.split("BODY:", 1)[-1].strip() if "BODY:" in full_text else full_text

          # Scrub out any leaked frontmatter lines from the top of the body
          parsed["BODY"] = re.sub(r'^(TITLE|SLUG|EXCERPT|CATEGORY|TAGS|IMAGE_KEYWORD|BODY):\s*.*$', '', parsed["BODY"], flags=re.MULTILINE | re.IGNORECASE).strip()

          parsed["BODY"] = dash_scrubber(parsed["BODY"]).strip()

          tags_list = [t.strip().replace('"', '') for t in parsed["TAGS"].split(",") if t.strip()]
          tags_str = ', '.join([f'"{t}"' for t in tags_list]) if tags_list else '"global", "news"'

          final_image_url = target_image or get_real_image(parsed["IMAGE_KEYWORD"] or target_title)

          final_file = textwrap.dedent(f"""\
          ---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{final_image_url}"
          category: "{parsed['CATEGORY'] or 'Global News'}"
          date: "{today_str}"
          tags: [{tags_str}]
          ---

          {parsed['BODY']}
          """)

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          filename = f"{parsed['SLUG']}.md"
          with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
              f.write(final_file)

          memory.append(final_hash)
          os.makedirs(os.path.dirname(memory_path), exist_ok=True)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)

          print(f"‚úÖ Saved: {filename} | Body chars: {len(parsed['BODY'])}")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh sports & tech scoop via direct links üåç"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'content/posts/*.md .github/scrape_memory.json'
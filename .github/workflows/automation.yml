name: International Gossip Bot (NewsAPI + Gemini)

on:
  schedule:
    - cron: '30 1 * * *'  # 4:30AM EAT
    - cron: '30 5 * * *'  # 8:30AM EAT
    - cron: '30 10 * * *' # 1:30PM EAT
    - cron: '30 13 * * *' # 4:30PM EAT
    - cron: '30 17 * * *' # 8:30PM EAT
    - cron: '30 21 * * *' # 12:30AM EAT
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic: celebrity, sports, technology, politics'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, sys
          from google import genai
          from google.genai import types

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          MODELS_TO_TRY = ["gemini-2.0-flash", "gemini-1.5-flash-001", "gemini-1.5-pro-001"]

          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani"
          ]

          # Topic rotation
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['celebrity', 'sports', 'technology', 'politics']:
              topic = manual_input
          else:
              hour_mod = current_hour % 6
              topics = ['celebrity', 'sports', 'technology', 'politics', 'celebrity', 'sports']
              topic = topics[hour_mod] if hour_mod < len(topics) else 'celebrity'
          
          cat_map = {'celebrity': 'entertainment', 'sports': 'sports', 'technology': 'technology', 'politics': 'general'}
          newsapi_cat = cat_map.get(topic, 'general')
          
          print(f"ðŸŽ¯ Topic: {topic} | NewsAPI: {newsapi_cat}")

          # --- 2. FETCH INTERNATIONAL NEWS ---
          params = {
              'category': newsapi_cat,
              'language': 'en',
              'pageSize': 15,
              'apiKey': os.environ['NEWSAPI_KEY']
          }
          try:
              news_resp = requests.get('https://newsapi.org/v2/top-headlines', params=params, timeout=10)
              news_data = news_resp.json()
          except Exception as e:
              print(f"Error fetching news: {e}")
              sys.exit(0)
          
          if news_data.get('status') != 'ok':
              print(f"âŒ NewsAPI Error: {news_data.get('message', 'Unknown error')}")
              sys.exit(0)

          if news_data.get('totalResults', 0) == 0:
              print("No fresh news, skipping")
              sys.exit(0)
          
          top_story = max(news_data['articles'], key=lambda x: x['publishedAt'] if x['publishedAt'] else '0000')
          title = top_story['title']
          desc = top_story.get('description') or ""
          content = top_story.get('content') or ""
          snippet = (desc + " " + content).strip()[:500]
          source_obj = top_story.get('source') or {}
          source = source_obj.get('name', 'Unknown Source')
          
          print(f"ðŸ“° NewsAPI Headline: {title[:80]}...")

          # --- 3. HELPERS ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback_image = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not access_key: return fallback_image
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except Exception: pass
              return fallback_image

          def dash_scrubber(text):
              # Replaces em dashes (\u2014) and en dashes (\u2013) with a standard hyphen
              return text.replace('\u2014', '-').replace('\u2013', '-')

          # --- 4. GROUNDED GENERATION ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{title}"
          SOURCE SNIPPET: "{snippet}"

          TASK: Research the LATEST developments of this story using Google Search to ensure your commentary is accurate as of {today_str}. 

          ROLE: "The Nairobi Gossip Source" - A hyper-local, savage commentator. Use Nairobi slang but NO banned phrases: {", ".join(BANNED_PHRASES)}.

          STRICT DASH RULE: NEVER use em dashes (â€”) or en dashes (â€“). Use regular hyphens (-) only.

          OUTPUT FORMAT:
          TITLE: [Savage Clickbait]
          SLUG: [url-friendly-lowercase]
          EXCERPT: [One spicy hook]
          CATEGORY: {topic.capitalize()}
          TAGS: [comma, separated, tags]
          IMAGE_KEYWORD: [search query]
          BODY:
          [Article text in Markdown. Start directly with the gossip.]
          '''

          full_text = ""
          success_flag = False
          
          for model_id in MODELS_TO_TRY:
              print(f"ðŸ¤– Attempting with {model_id}...")
              try:
                  response = client.models.generate_content(
                      model=model_id,
                      contents=prompt,
                      config=types.GenerateContentConfig(
                          temperature=0.9,
                          tools=[google_search_tool]
                      )
                  )
                  full_text = response.text.strip()
                  # Apply Scrubber immediately
                  full_text = dash_scrubber(full_text)
                  success_flag = True
                  print(f"âœ… SUCCESS with {model_id}!")
                  break 
              except Exception as e:
                  print(f"âš ï¸ Error with {model_id}: {e}")
                  time.sleep(10)
                  continue

          if not success_flag: sys.exit(1)

          # --- 5. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          
          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """
          
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{parsed['SLUG']}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          
          print(f"âœ… File saved: {parsed['SLUG']}.md")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh gossip â˜•"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Gossip Bot"
          commit_user_email: "actions@github.com"
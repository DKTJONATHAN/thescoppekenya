name: International Gossip Bot (NewsAPI + Gemini)

on:
  schedule:
    - cron: '30 1 * * *'  # 4:30AM EAT
    - cron: '30 5 * * *'  # 8:30AM EAT
    - cron: '30 10 * * *' # 1:30PM EAT
    - cron: '30 13 * * *' # 4:30PM EAT
    - cron: '30 17 * * *' # 8:30PM EAT
    - cron: '30 21 * * *' # 12:30AM EAT
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic: celebrity, sports, technology, politics'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, sys
          from google import genai
          from google.genai import types

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          # FIXED MODEL LIST
          # 1. Try 2.0 Flash (Fastest)
          # 2. Fallback to 1.5 Flash 001 (Strict ID to fix 404)
          # 3. Fallback to 1.5 Pro (Most reliable)
          MODELS_TO_TRY = ["gemini-2.0-flash", "gemini-1.5-flash-001", "gemini-1.5-pro-001"]

          # BANNED PHRASES
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani"
          ]

          # Topic rotation
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['celebrity', 'sports', 'technology', 'politics']:
              topic = manual_input
          else:
              hour_mod = current_hour % 6
              topics = ['celebrity', 'sports', 'technology', 'politics', 'celebrity', 'sports']
              if hour_mod < len(topics):
                  topic = topics[hour_mod]
              else:
                  topic = 'celebrity'
          
          cat_map = {'celebrity': 'entertainment', 'sports': 'sports', 'technology': 'technology', 'politics': 'general'}
          newsapi_cat = cat_map.get(topic, 'general')
          
          print(f"ðŸŽ¯ Topic: {topic} | NewsAPI: {newsapi_cat}")

          # --- 2. FETCH INTERNATIONAL NEWS ---
          params = {
              'category': newsapi_cat,
              'language': 'en',
              'pageSize': 15,
              'apiKey': os.environ['NEWSAPI_KEY']
          }
          try:
              news_resp = requests.get('https://newsapi.org/v2/top-headlines', params=params, timeout=10)
              news_data = news_resp.json()
          except Exception as e:
              print(f"Error fetching news: {e}")
              sys.exit(0)
          
          if news_data.get('totalResults', 0) == 0:
              print("No fresh news, skipping")
              sys.exit(0)
          
          # Use MAX to get the NEWEST story
          top_story = max(news_data['articles'], key=lambda x: x['publishedAt'] if x['publishedAt'] else '0000')
          
          title = top_story['title']
          snippet = (top_story.get('description') or top_story.get('content', ''))[:300]
          source = top_story['source']['name']
          
          print(f"ðŸ“° Freshest Headline: {title[:80]}... ({source})")

          # --- 3. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback_image = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not access_key: return fallback_image
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except Exception: pass
              return fallback_image

          # --- 4. GROUNDED GENERATION WITH PATIENCE ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))

          google_search_tool = types.Tool(
              google_search=types.GoogleSearch()
          )

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{title}"
          SOURCE SNIPPET: "{snippet}"

          ROLE: "The Nairobi Gossip Source" - A hyper-local, savage commentator.

          TASK 1 (RESEARCH):
          - Use Google Search to find the *latest* details on this story (verify if it's trending *now*).
          - Use Google Search to find 3-5 *current* trending slang words in Kenya (Nairobi) for {today_str}.

          TASK 2 (WRITE):
          Write a savage gossip article about this news.
          
          STRICT STYLE RULES:
          1. **NO REPETITION**: Do NOT use these banned words: {", ".join(BANNED_PHRASES)}.
          2. **FRESH SLANG**: Use the *new* slang you found in Task 1.
          3. **TONE**: Brutal, funny, distinctively Kenyan (mix English with fresh Sheng).
          4. **RECENCY**: If the news is old/stale, roast the person for "bringing up old files" or find a new angle.

          OUTPUT FORMAT:
          TITLE: [Savage Clickbait]
          SLUG: [url-friendly-lowercase]
          EXCERPT: [One spicy hook]
          CATEGORY: {topic.capitalize()}
          TAGS: [comma, separated, tags]
          IMAGE_KEYWORD: [search query]
          BODY:
          [1000 words. Markdown. 4 Sections. Brutal honesty.]
          '''

          full_text = ""
          success_flag = False
          
          for model_id in MODELS_TO_TRY:
              print(f"ðŸ¤– Attempting with {model_id}...")
              
              # Inner retry loop for Quota (429) errors
              max_quota_retries = 2
              for q_attempt in range(max_quota_retries):
                  try:
                      response = client.models.generate_content(
                          model=model_id,
                          contents=prompt,
                          config=types.GenerateContentConfig(
                              temperature=0.9,
                              tools=[google_search_tool],
                              response_modalities=["TEXT"]
                          )
                      )
                      
                      # Metadata Check
                      if not response.candidates[0].grounding_metadata.search_entry_point:
                          print(f"âš ï¸ {model_id} didn't search. Trying next model...")
                          break # Break inner loop, go to next model

                      full_text = response.text.strip()
                      print(f"âœ… SUCCESS with {model_id}!")
                      success_flag = True
                      break # Break inner loop
                  
                  except Exception as e:
                      error_str = str(e)
                      if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                          print(f"â³ Quota hit on {model_id}. Sleeping 70s to cool down...")
                          time.sleep(70) # WAIT for quota reset
                          continue # Retry same model
                      elif "404" in error_str:
                          print(f"âŒ Model {model_id} not found. Skipping.")
                          break # Break inner loop, try next model
                      else:
                          print(f"âš ï¸ Error with {model_id}: {e}")
                          break # Break inner loop, try next model
              
              if success_flag: break

          if not success_flag:
              print("âŒ All models failed after retries.")
              sys.exit(1)

          # --- 5. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          # Fallbacks
          if not parsed["TITLE"]: parsed["TITLE"] = f"{topic.capitalize()} Update: {title[:20]}"
          if not parsed["SLUG"]: parsed["SLUG"] = re.sub(r'[^a-z0-9-]', '-', title.lower())[:80].strip('-')

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          
          import textwrap
          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY']}
          """
          
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{parsed['SLUG']}.md"), "w", encoding="utf-8") as f:
              f.write(textwrap.dedent(final_file).strip())
          
          print(f"âœ… Tea spilled: {parsed['SLUG']}.md")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh gossip â˜•"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Gossip Bot"
          commit_user_email: "actions@github.com"
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
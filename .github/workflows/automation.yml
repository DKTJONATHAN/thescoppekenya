- name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, sys
          from google import genai
          from google.genai import types

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          # Banned words list (The "Old Slang" Filter)
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani" # Add any other repetitive ones here
          ]

          # Topic rotation
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['celebrity', 'sports', 'technology', 'politics']:
              topic = manual_input
          else:
              hour_mod = current_hour % 6
              topics = ['celebrity', 'sports', 'technology', 'politics', 'celebrity', 'sports']
              if hour_mod < len(topics):
                  topic = topics[hour_mod]
              else:
                  topic = 'celebrity'
          
          cat_map = {'celebrity': 'entertainment', 'sports': 'sports', 'technology': 'technology', 'politics': 'general'}
          newsapi_cat = cat_map.get(topic, 'general')
          
          print(f"ðŸŽ¯ Topic: {topic} | NewsAPI: {newsapi_cat}")

          # --- 2. FETCH INTERNATIONAL NEWS (FIXED SORTING) ---
          params = {
              'category': newsapi_cat,
              'language': 'en',
              'pageSize': 15, # Increased pool size
              'apiKey': os.environ['NEWSAPI_KEY']
          }
          try:
              news_resp = requests.get('https://newsapi.org/v2/top-headlines', params=params, timeout=10)
              news_data = news_resp.json()
          except Exception as e:
              print(f"Error fetching news: {e}")
              sys.exit(0)
          
          if news_data.get('totalResults', 0) == 0:
              print("No fresh news, skipping")
              sys.exit(0)
          
          # CRITICAL FIX: 'max' picks the NEWEST date. 'min' was picking the oldest.
          top_story = max(news_data['articles'], key=lambda x: x['publishedAt'] if x['publishedAt'] else '0000')
          
          title = top_story['title']
          snippet = (top_story.get('description') or top_story.get('content', ''))[:300]
          source = top_story['source']['name']
          pub_date = top_story['publishedAt']
          
          print(f"ðŸ“° Freshest Headline: {title[:80]}... ({source})")

          # --- 3. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback_image = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not access_key: return fallback_image
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except Exception: pass
              return fallback_image

          # --- 4. GROUNDED GENERATION (LIVE SEARCH) ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          model_id = "gemini-1.5-flash" # Best balance of speed/search

          # This tool enables live Google Search
          google_search_tool = types.Tool(
              google_search=types.GoogleSearch()
          )

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{title}"
          SOURCE SNIPPET: "{snippet}"

          ROLE: "The Nairobi Gossip Source" - A hyper-local, savage commentator.

          TASK 1 (RESEARCH):
          - Use Google Search to find the *latest* details on this story (verify if it's trending *now*).
          - Use Google Search to find 3-5 *current* trending slang words in Kenya (Nairobi) for {today_str}.

          TASK 2 (WRITE):
          Write a savage gossip article about this news.
          
          STRICT STYLE RULES:
          1. **NO REPETITION**: Do NOT use these banned words: {", ".join(BANNED_PHRASES)}.
          2. **FRESH SLANG**: Use the *new* slang you found in Task 1.
          3. **TONE**: Brutal, funny, distinctively Kenyan (mix English with fresh Sheng).
          4. **RECENCY**: If the news is old/stale, roast the person for "bringing up old files" or find a new angle.

          OUTPUT FORMAT:
          TITLE: [Savage Clickbait]
          SLUG: [url-friendly-lowercase]
          EXCERPT: [One spicy hook]
          CATEGORY: {topic.capitalize()}
          TAGS: [comma, separated, tags]
          IMAGE_KEYWORD: [search query]
          BODY:
          [1000 words. Markdown. 4 Sections. Brutal honesty.]
          '''

          print(f"ðŸ¤– Searching & Writing with {model_id}...")
          
          full_text = ""
          max_retries = 3
          
          for attempt in range(max_retries):
              try:
                  response = client.models.generate_content(
                      model=model_id,
                      contents=prompt,
                      config=types.GenerateContentConfig(
                          temperature=0.9, # High creativity for slang
                          tools=[google_search_tool], # <--- LIVE SEARCH ENABLED
                          response_modalities=["TEXT"]
                      )
                  )
                  
                  # Check if search actually happened (Grounding Check)
                  if not response.candidates[0].grounding_metadata.search_entry_point:
                      print("âš ï¸ Warning: Model didn't search. Retrying...")
                      raise Exception("Grounding failure")

                  full_text = response.text.strip()
                  print("âœ… Generation successful (with Live Search)!")
                  break
              except Exception as e:
                  print(f"âš ï¸ Attempt {attempt + 1} failed: {e}")
                  if attempt < max_retries - 1:
                      time.sleep(5)
                  else:
                      print("âŒ All retries failed.")
                      sys.exit(1)

          # --- 5. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          # Fallbacks
          if not parsed["TITLE"]: parsed["TITLE"] = f"{topic.capitalize()} Update: {title[:20]}"
          if not parsed["SLUG"]: parsed["SLUG"] = re.sub(r'[^a-z0-9-]', '-', title.lower())[:80].strip('-')

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          
          import textwrap
          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY']}
          """
          
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{parsed['SLUG']}.md"), "w", encoding="utf-8") as f:
              f.write(textwrap.dedent(final_file).strip())
          
          print(f"âœ… Tea spilled: {parsed['SLUG']}.md")
          EOF
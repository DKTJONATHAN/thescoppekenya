name: Mpasho Scraper (Guaranteed Images)

on:
  schedule:
    # Runs every 2 hours and 17 minutes between 7 AM and 10 PM
    - cron: '0 4 * * *'
    - cron: '17 6 * * *'
    - cron: '34 8 * * *'
    - cron: '51 10 * * *'
    - cron: '8 13 * * *'
    - cron: '25 15 * * *'
    - cron: '42 17 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install -U google-genai requests beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Run Scraper
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          POSTS_DIR: content/posts
        run: |
          python << 'EOF'
          import os, random, sys, time, requests, re
          from google import genai
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright
          from datetime import datetime, timedelta
          from email.utils import parsedate_to_datetime

          # --- CONFIGURATION ---
          RSS_FEED = "https://news.google.com/rss/search?q=site:mpasho.co.ke+when:12h&hl=en-KE&gl=KE&ceid=KE:en"
          HISTORY_FILE = "posted_history.txt"

          # --- DASH CLEANER ---
          def clean_dashes(text):
              if not text: return ""
              text = text.replace('‚Äî', ' - ').replace('‚Äì', '-')
              text = re.sub(r'\s+', ' ', text)
              return text.strip()

          # --- HISTORY MANAGER ---
          def load_history():
              if not os.path.exists(HISTORY_FILE): return set()
              with open(HISTORY_FILE, "r") as f:
                  return set(line.strip() for line in f.readlines())

          def save_to_history(link):
              with open(HISTORY_FILE, "a") as f:
                  f.write(f"{link}\n")

          # --- URL DECODER (BYPASS GOOGLE NEWS 403) ---
          def resolve_google_news_url(url):
              print(f"üîó Decoding Google News URL: {url}")
              try:
                  from urllib.parse import urlparse
                  import json
                  
                  # Extract the base64 token from the end of the URL
                  guid = urlparse(url).path.split('/')[-1]
                  
                  # Build Google's internal batchexecute RPC payload
                  param = f'["garturlreq",[["en-US","US",["FINANCE_TOP_INDICES","WEB_TEST_1_0_0"],null,null,1,1,"US:en",null,null,null,null,null,null,null,0,5],"en-US","US",true,[2,4,8],1,true,"661099999",0,0,null,0],"{guid}"]'
                  data = {'f.req': json.dumps([[["Fbv4je", param, "null", "generic"]]])}
                  
                  headers = {
                      'content-type': 'application/x-www-form-urlencoded;charset=UTF-8',
                      'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'
                  }
                  
                  # Hit the hidden endpoint
                  res = requests.post("https://news.google.com/_/DotsSplashUi/data/batchexecute", data=data, headers=headers, timeout=10)
                  
                  # Extract the clean URL using regex
                  match = re.search(r'\\"garturlres\\",\\"(https?://[^\\]+)\\"', res.text)
                  if match:
                      final_url = match.group(1)
                      print(f"‚úÖ Decoded successfully: {final_url}")
                      return final_url
                      
                  print("‚ö†Ô∏è Decode failed. Returning original URL.")
                  return url
              except Exception as e:
                  print(f"‚ö†Ô∏è URL Decode Error: {e}")
                  return url

          # --- GOOGLE NEWS FETCH ---
          def get_fresh_links():
              print(f"üì° Checking Google News (Last 12h)...")
              history = load_history()
              try:
                  resp = requests.get(RSS_FEED, timeout=10)
                  soup = BeautifulSoup(resp.content, "xml")
                  items = soup.find_all("item")
                  
                  valid_links = []
                  for item in items:
                      link = item.link.text.strip()
                      pub_str = item.pubDate.text.strip()
                      
                      if link in history:
                          continue
                      
                      # Check freshness (must be < 24h old)
                      try:
                          pdate = parsedate_to_datetime(pub_str)
                          if datetime.now(pdate.tzinfo) - pdate > timedelta(hours=24):
                              continue
                      except: pass

                      valid_links.append(link)

                  print(f"‚úÖ Found {len(valid_links)} NEW articles.")
                  return valid_links
              except Exception as e:
                  print(f"‚ùå RSS Error: {e}")
                  return []

          # --- PLAYWRIGHT SCRAPER ---
          def scrape_article_universal(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
                  context = browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
                  )
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

                  try:
                      page.goto(url, timeout=60000)
                      
                      # Handle Google Consent if the decoder failed and we got stuck
                      if "consent.google.com" in page.url or "news.google.com" in page.url:
                          try:
                              page.locator('button:has-text("Accept all")').click(timeout=5000)
                          except:
                              pass

                      try: page.wait_for_load_state("networkidle", timeout=15000)
                      except: pass

                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      
                      h1 = soup.find("h1")
                      title = h1.text.strip() if h1 else soup.title.string

                      article_text = ""
                      # Priority Containers
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          txt = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(txt) > 500:
                              article_text = txt
                              break
                      
                      # Fallback Vacuum
                      if not article_text:
                          all_p = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_p if len(p.text) > 60])

                      browser.close()
                      return title, article_text
                  except:
                      browser.close()
                      return None, None

          # --- MAIN LOGIC ---
          candidates = get_fresh_links()
          if not candidates:
              print("No fresh news.")
              sys.exit(0)

          target_link = candidates[0]
          # Bypassing the Google News redirect wall
          resolved_link = resolve_google_news_url(target_link)
          
          final_title, final_text = scrape_article_universal(resolved_link)

          if not final_text or len(final_text) < 300:
              print("‚ùå Scrape failed.")
              sys.exit(1)

          # --- AI REWRITE ---
          print("ü§ñ Rewriting...")
          keys = [os.environ.get("GEMINI_WRITE_KEY"), os.environ.get("GEMINI_API_KEY")]
          keys = [k for k in keys if k]
          models = ["gemini-2.0-flash", "gemini-2.5-flash"]

          prompt = f"""
          Rewrite this for a Nairobi Gossip Blog (Sheng).
          Title: {final_title}
          Body: {final_text[:5000]}
          
          Format:
          TITLE: [Sheng Title]
          SLUG: [slug]
          EXCERPT: [Hook]
          CATEGORY: Entertainment
          TAGS: Gossip, Nairobi
          IMAGE_KEYWORD: [One single broad English noun e.g. City, Crowd, Money, Car. Do NOT use names.]
          BODY:
          [Markdown Text]
          """

          success = False
          final_output = ""
          
          for k in keys:
              client = genai.Client(api_key=k)
              for m in models:
                  try:
                      resp = client.models.generate_content(
                          model=m,
                          contents=prompt
                      )
                      final_output = resp.text
                      success = True
                      break
                  except Exception as e:
                      print(f"‚ö†Ô∏è Model {m} failed: {e}")
                      time.sleep(1)
              if success: break

          if not success: 
              print("‚ùå All Gemini generations failed.")
              sys.exit(1)

          # --- SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          curr = None
          
          for line in final_output.splitlines():
              line = clean_dashes(line)
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  curr = "BODY"
                  continue
              elif curr == "BODY":
                  parsed["BODY"] += line + "\n"
          
          parsed["BODY"] = clean_dashes(parsed["BODY"])

          # --- ROBUST IMAGE FINDER ---
          def get_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              # Guaranteed Fallback Image (Nairobi Skyline)
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              
              if not access_key: return fallback

              # Try 3 layers of keywords: Specific -> City -> Country
              search_terms = [query, "Nairobi", "Kenya"]
              # Filter out empty terms
              search_terms = [t for t in search_terms if t and len(t) > 2]

              for term in search_terms:
                  try:
                      print(f"üì∑ Trying image search: {term}")
                      u = f"https://api.unsplash.com/photos/random?query={term}&orientation=landscape&client_id={access_key}"
                      r = requests.get(u, timeout=5)
                      if r.status_code == 200:
                          data = r.json()
                          # Unsplash API sometimes returns list, sometimes dict
                          if isinstance(data, list) and data:
                              return data[0]['urls']['regular']
                          elif isinstance(data, dict) and 'urls' in data:
                              return data['urls']['regular']
                  except Exception as e:
                      print(f"   Image Error ({term}): {e}")
              
              print("‚ö†Ô∏è All image searches failed. Using fallback.")
              return fallback

          image_url = get_image(parsed['IMAGE_KEYWORD'])
          today = datetime.utcnow().strftime("%Y-%m-%d")

          md = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """

          path = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"), f"{parsed['SLUG']}.md")
          with open(path, "w", encoding="utf-8") as f:
              f.write(md)
          
          save_to_history(target_link)
          print(f"‚úÖ Published: {parsed['SLUG']}.md")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh scoop üïµÔ∏è"
          branch: main
          file_pattern: "content/posts/*.md posted_history.txt"
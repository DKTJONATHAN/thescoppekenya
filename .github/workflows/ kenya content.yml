name: Mpasho Scraper (Google News Bypass)

on:
  schedule:
    - cron: '0 6,12,18 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install playwright google-genai beautifulsoup4 lxml requests
          playwright install chromium

      - name: Run Stealth Scraper
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          POSTS_DIR: content/posts
        run: |
          python << 'EOF'
          import os, random, sys, time, requests
          from google import genai
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright
          from datetime import datetime

          # --- CONFIGURATION ---
          # We search Google News specifically for Mpasho articles from the last 1 day
          RSS_FEED = "https://news.google.com/rss/search?q=site:mpasho.co.ke+when:1d&hl=en-KE&gl=KE&ceid=KE:en"

          def get_links_from_google():
              print(f"üì° Asking Google News for Mpasho links...")
              try:
                  # Google News is easy to scrape with basic requests
                  resp = requests.get(RSS_FEED, timeout=10)
                  soup = BeautifulSoup(resp.content, "xml")
                  items = soup.find_all("item")
                  
                  links = []
                  for item in items:
                      # Google News links look like "https://news.google.com/rss/articles/..."
                      # Playwright will follow the redirect to the real Mpasho link
                      links.append(item.link.text)

                  print(f"‚úÖ Google found {len(links)} articles.")
                  return links
              except Exception as e:
                  print(f"‚ùå Google RSS Error: {e}")
                  return []

          def scrape_article_stealth(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              
              with sync_playwright() as p:
                  # LAUNCH ARGS TO HIDE "HEADLESS" STATUS
                  browser = p.chromium.launch(
                      headless=True,
                      args=[
                          "--disable-blink-features=AutomationControlled",
                          "--no-sandbox",
                          "--disable-setuid-sandbox",
                          "--disable-infobars",
                          "--window-position=0,0",
                          "--ignore-certifcate-errors",
                          "--ignore-certifcate-errors-spki-list",
                          "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                      ]
                  )
                  
                  context = browser.new_context(
                      viewport={"width": 1366, "height": 768},
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                  )
                  
                  page = context.new_page()

                  # STEALTH: Remove the "webdriver" property that alerts Cloudflare
                  page.add_init_script("""
                      Object.defineProperty(navigator, 'webdriver', {
                          get: () => undefined
                      });
                  """)

                  try:
                      # Navigate
                      page.goto(url, timeout=90000)
                      
                      # Wait for redirect to finish and body to load
                      page.wait_for_load_state("domcontentloaded")
                      
                      # Random mouse jiggle to look human
                      page.mouse.move(100, 100)
                      page.wait_for_timeout(1000)
                      page.mouse.move(200, 200)

                      # Grab content
                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      
                      # DEBUG: Print Title to verify we are on Mpasho
                      page_title = soup.title.string if soup.title else "No Title"
                      print(f"   Landed on: {page_title}")

                      if "Just a moment" in page_title or "Cloudflare" in page_title:
                          print("‚ùå Still blocked by Cloudflare challenge.")
                          browser.close()
                          return None, None

                      # EXTRACT BODY
                      # Mpasho Selectors
                      body_div = soup.find("div", class_="article-body") or \
                                 soup.find("div", class_="entry-content") or \
                                 soup.find("div", class_="post-content")

                      if not body_div:
                          print("‚ö†Ô∏è Loaded page but couldn't find text div.")
                          browser.close()
                          return None, None

                      paragraphs = body_div.find_all("p")
                      text = "\n\n".join([p.text.strip() for p in paragraphs])
                      
                      # EXTRACT TITLE
                      h1 = soup.find("h1")
                      title = h1.text.strip() if h1 else page_title

                      browser.close()
                      return title, text

                  except Exception as e:
                      print(f"‚ùå Playwright Error: {e}")
                      browser.close()
                      return None, None

          # --- MAIN EXECUTION ---
          
          # 1. Get Links from Google (Bypasses Homepage Block)
          candidates = get_links_from_google()
          if not candidates:
              sys.exit(0)
          
          # 2. Pick a random one
          # We try up to 3 links in case one fails
          final_title = None
          final_text = None
          
          # Shuffle to keep it random
          random.shuffle(candidates)
          
          for link in candidates[:3]:
              final_title, final_text = scrape_article_stealth(link)
              if final_text and len(final_text) > 300:
                  break # Success!
              print("   Retrying next link...")

          if not final_text:
              print("‚ùå All attempts failed.")
              sys.exit(1)

          print(f"üéØ Success! Scraped: {final_title}")

          # --- 3. REWRITE (Sheng Reporter) ---
          print("ü§ñ Rewriting with Gemini...")
          client = genai.Client(api_key=os.environ["GEMINI_WRITE_KEY"])
          
          prompt = f"""
          Rewrite this story into a Nairobi Gossip Blog post (Sheng).
          Title: {final_title}
          Body: {final_text[:6000]}
          
          Format:
          TITLE: [Sheng Clickbait Title]
          SLUG: [slug]
          EXCERPT: [Hook]
          CATEGORY: Entertainment
          TAGS: Gossip, Nairobi
          IMAGE_KEYWORD: [Search Term]
          BODY:
          [Markdown Text]
          """
          
          try:
              resp = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
              final_text = resp.text
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              sys.exit(1)

          # --- 4. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          
          for line in final_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = clean.replace("EXCERPT:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  continue
              elif current_section == "BODY":
                  parsed["BODY"] += line + "\n"

          def get_image(query):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              if not key: return fallback
              try:
                  u = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={key}"
                  r = requests.get(u, timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          image_url = get_image(parsed['IMAGE_KEYWORD'] or "Nairobi")
          today_str = datetime.utcnow().strftime("%Y-%m-%d")

          md_content = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          filename = f"{parsed['SLUG']}.md"
          
          with open(os.path.join(out_dir, filename), "w", encoding="utf-8") as f:
              f.write(md_content)
          
          print(f"‚úÖ Published: {filename}")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: mpasho stealth scoop üïµÔ∏è"
          branch: main
          file_pattern: content/posts/*.md
name: Kenya News British Gossip Scraper (Google News + Internal Links)

on:
  schedule:
    # 12 runs distributed from 6:00 AM EAT to 10:30 PM EAT
    - cron: '0 3 * * *'   # 6:00 AM EAT (3:00 AM UTC)
    - cron: '30 4 * * *'  # 7:30 AM EAT (4:30 AM UTC)
    - cron: '0 6 * * *'   # 9:00 AM EAT (6:00 AM UTC)
    - cron: '30 7 * * *'  # 10:30 AM EAT (7:30 AM UTC)
    - cron: '0 9 * * *'   # 12:00 PM EAT (9:00 AM UTC)
    - cron: '30 10 * * *' # 1:30 PM EAT (10:30 AM UTC)
    - cron: '0 12 * * *'  # 3:00 PM EAT (12:00 PM UTC)
    - cron: '30 13 * * *' # 4:30 PM EAT (1:30 PM UTC)
    - cron: '0 15 * * *'  # 6:00 PM EAT (3:00 PM UTC)
    - cron: '30 16 * * *' # 7:30 PM EAT (4:30 PM UTC)
    - cron: '0 18 * * *'  # 9:00 PM EAT (6:00 PM UTC)
    - cron: '30 19 * * *' # 10:30 PM EAT (7:30 PM UTC)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic: entertainment, politics, news, sports, business, tech'
        required: false
        default: ''

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: ${{ env.POSTS_DIR }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, time, sys, hashlib, random, urllib.parse, textwrap
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright

          # --- 1. CONFIGURATION ---
          today_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
          full_date_str = datetime.datetime.utcnow().strftime("%A, %B %d, %Y")
          current_hour = datetime.datetime.utcnow().hour

          MODELS_TO_TRY = ["gemini-3-flash-preview", "gemini-2.5-flash"]
          
          # Humanizer: added AI cliches and old slang to banned list
          BANNED_PHRASES = [
              "sasa basi", "melting the pot", "spill the tea", "tea is hot", 
              "grab your popcorn", "listen up", "buckle up", "breaking news",
              "sherehe", "form ni gani", "udaku", "hello guys", "welcome back",
              "mambo vipi", "niaje wasee", "karibuni", "tapestry", "dive in",
              "delve into", "moreover", "furthermore", "in conclusion", 
              "it's worth noting", "a testament to", "navigating the landscape",
              "in today's digital age", "shed light on"
          ]

          # DYNAMIC OPENINGS TO PREVENT REPETITION (British Gossip & Psychological Hooks)
          OPENING_VIBES = [
              "Start with a cheeky rhetorical question asking the reader if they are ready for this mess.",
              "Start by pretending to whisper a secret you just found out.",
              "Start with a blunt, slightly sarcastic summary of the situation.",
              "Start by acting exhausted by the constant drama but admitting you absolutely love it.",
              "Start with a direct address to the reader like you are chatting over a cup of coffee.",
              "Start by expressing shock at how cheeky people are being online today.",
              "Start with a relatable, slightly embarrassing personal anecdote that somehow connects to the news.",
              "Start with a bold, controversial statement that challenges a popular opinion, but keep it lighthearted.",
              "Start by highlighting a shocking or surprising fact about the situation that most people completely missed.",
              "Start with a vivid, dramatic metaphor, like comparing the drama to a telenovela or a messy breakup.",
              "Start by triggering FOMO (Fear Of Missing Out), making the reader feel they are the last to know this crucial gossip."
          ]
          random_hook = random.choice(OPENING_VIBES)

          hour_to_topic = {
              3: 'entertainment', 4: 'news', 6: 'entertainment', 7: 'politics',
              9: 'entertainment', 10: 'sports', 12: 'entertainment', 13: 'business',
              15: 'entertainment', 16: 'tech', 18: 'entertainment', 19: 'news'
          }

          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          valid_topics = ['entertainment', 'politics', 'news', 'sports', 'business', 'tech']
          
          if manual_input in valid_topics:
              topic = manual_input
          else:
              topic = hour_to_topic.get(current_hour, 'entertainment')
          
          print(f"üéØ Topic: {topic}")

          # --- 2. FETCH GOOGLE NEWS RSS (KENYA) ---
          base_keywords = {
              'entertainment': 'kenya entertainment OR kenya celebrity OR nairobi gossip OR kenya showbiz',
              'politics': 'kenya politics OR kenya parliament OR nairobi politics OR kenya government',
              'news': 'kenya breaking news OR nairobi news OR kenya latest news',
              'sports': 'kenya sports OR kenya athletics OR kenya football OR nairobi sports',
              'business': 'kenya economy OR kenya business OR kenya finance OR nairobi markets',
              'tech': 'kenya tech OR kenya technology OR nairobi startup OR kenya innovation'
          }
          query_str = base_keywords.get(topic, 'kenya breaking news')
          safe_query = urllib.parse.quote(f"{query_str} when:24h")
          
          rss_url = f"https://news.google.com/rss/search?q={safe_query}&hl=en-KE&gl=KE&ceid=KE:en"
          print(f"üåç Fetching Google News: {rss_url}")
          
          try:
              resp = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(resp.content, "xml")
              articles = [{"title": item.title.text, "url": item.link.text} for item in soup.find_all("item")]
          except Exception as e:
              print(f"Error fetching Google News: {e}")
              sys.exit(0)

          if not articles:
              print("No fresh news, skipping")
              sys.exit(0)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          # --- 3. FREE SCRAPER ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
                  context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36")
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                  try:
                      page.goto(url, timeout=50000)
                      try: page.wait_for_load_state("networkidle", timeout=10000)
                      except: pass
                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      img_url = ""
                      og_image = soup.find("meta", property="og:image")
                      if og_image and og_image.get("content"): img_url = og_image["content"]
                      if not img_url:
                          twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
                          if twitter_image and twitter_image.get("content"): img_url = twitter_image["content"]
                      
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 600:
                              article_text = chunk
                              break
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                      return article_text, img_url
                  except: return None, None
                  finally: browser.close()

          # --- 4. PROCESS ARTICLES ---
          full_raw_text, final_hash, target_title, target_image = None, None, None, None
          for a in articles:
              u_hash = hashlib.md5(a['url'].encode()).hexdigest()
              if u_hash in memory: continue
              result = scrape_article_free(a['url'])
              if not result: continue
              text, img = result
              if text and len(text) > 600:
                  full_raw_text = text
                  target_image = img
                  final_hash = u_hash
                  target_title = re.sub(r' - [^-]+$', '', a['title']).strip()
                  break 

          if not full_raw_text: 
              print("‚ùå No valid unread content found.")
              sys.exit(0)

          # --- 5. HELPERS ---
          def get_internal_context(posts_dir):
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST STORIES (PICK ONE AND LINK IT NATURALLY IN THE TEXT USING MARKDOWN):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r', encoding='utf-8') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- Title: {t} | Link: https://zandani.co.ke/posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200"
              if not access_key: return fallback
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200: return resp.json()['urls']['regular']
              except: pass
              return fallback

          def dash_scrubber(text):
              if not text: return ""
              text = text.replace('\u2014', '-').replace('\u2013', '-')
              text = text.replace(chr(8212), '-').replace(chr(8211), '-')
              return text

          # --- 6. GENERATION ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          google_search_tool = types.Tool(google_search=types.GoogleSearch())
          internal_links_data = get_internal_context(os.environ.get("POSTS_DIR", "content/posts"))

          prompt = f'''Current Date: {full_date_str}
          NEWS HEADLINE: "{target_title}"
          SOURCE SCRAPED TEXT: "{full_raw_text[:12000]}"
          
          {internal_links_data}

          TASK: Write a news report as a **British Tabloid Gossip Columnist**.
          1. Use Google Search to research the latest details on this story if needed.
          2. Naturally insert a markdown link to one of the PAST STORIES provided above somewhere in the body paragraph.

          LANGUAGE MANDATE (STRICT):
          - NO SHENG OR SWAHILI. You are writing for an audience who loves simple, dramatic British gossip.
          - Use very simple, conversational British English. Think of a chatty, cheeky tabloid reporter having a chinwag.
          - Keep the vocabulary basic but the tone highly dramatic and gossipy. 
          - Use conversational fillers naturally to sound human: "Right,", "Well,", "Honestly,", "Anyway,", "Mate."
          - Include rhetorical questions and subjective asides in brackets (e.g., "Can you even imagine?", "What a nightmare.").
          - Vary sentence length aggressively. Mix short, punchy facts with longer, breathy gossip thoughts.
          - BANNED PHRASES: {", ".join(BANNED_PHRASES)}.
          - NEVER use long dashes or double dashes. Only use regular single hyphens.

          ANTI-REPETITION RULES:
          - DO NOT use the exact same words or structure as previous articles. 
          - HERE IS YOUR UNIQUE OPENING INSTRUCTION FOR THIS SPECIFIC ARTICLE: {random_hook}

          EXAMPLE OF THE PERFECT TONE VIBE TO MIMIC (DO NOT COPY THESE EXACT WORDS):
          "Right, gather round. If you thought yesterday was dramatic, you are in for an absolute treat. The internet is having a right laugh about this one. So, here is what actually went down..."

          STRICT OUTPUT FORMAT:
          TITLE: [Very Simple SEO-Optimized English Title starting with the main keyword]
          SLUG: [url-friendly-lowercase-english-slug]
          EXCERPT: [Simple SEO-Optimized English Snippet/Hook under 160 characters]
          CATEGORY: Kenya News
          TAGS: [comma, separated, english, seo, tags]
          IMAGE_KEYWORD: [search query]
          BODY:
          [Article text in chatty, simple British English containing the internal link. 
          Structure it logically: 
          - Include an H2 heading asking a direct question about the core gossip/news.
          - Immediately follow that H2 with a crisp, 40 to 60 word neutral answer paragraph to win Google Featured Snippets.
          - Then dive into the rest of the cheeky British gossip: The Scoop -> The Evidence -> The Verdict.]
          '''

          full_text = ""
          success_flag = False
          
          for model_id in MODELS_TO_TRY:
              for retry in range(2):
                  try:
                      response = client.models.generate_content(
                          model=model_id, contents=prompt,
                          config=types.GenerateContentConfig(temperature=0.9, tools=[google_search_tool])
                      )
                      full_text = dash_scrubber(response.text.strip())
                      success_flag = True
                      print(f"‚úÖ SUCCESS with {model_id}!")
                      break 
                  except Exception as e:
                      if "429" in str(e):
                          print("‚è≥ Quota hit. Waiting 35s...")
                          time.sleep(35)
                          continue
                      break
              if success_flag: break

          if not success_flag: sys.exit(1)

          # --- 7. PARSE & SAVE ---
          parsed = { "TITLE": "", "SLUG": "", "EXCERPT": "", "CATEGORY": "", "TAGS": "", "IMAGE_KEYWORD": "", "BODY": "" }
          current_section = None
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("```"): continue
              if clean.startswith("TITLE:"): parsed["TITLE"] = dash_scrubber(clean.replace("TITLE:", "").strip())
              elif clean.startswith("SLUG:"): parsed["SLUG"] = clean.replace("SLUG:", "").strip()
              elif clean.startswith("EXCERPT:"): parsed["EXCERPT"] = dash_scrubber(clean.replace("EXCERPT:", "").strip())
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"):
                  current_section = "BODY"
                  inline_body = line.split("BODY:", 1)[-1].strip()
                  if inline_body: parsed["BODY"] += inline_body + "\n"
                  continue
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          final_image_url = target_image if target_image else get_real_image(parsed['IMAGE_KEYWORD'] or target_title)
          
          final_file = textwrap.dedent(f"""\
          ---
          title: "{parsed['TITLE'].replace('"', "'")}"
          slug: "{parsed['SLUG']}"
          excerpt: "{parsed['EXCERPT'].replace('"', "'")}"
          image: "{final_image_url}"
          category: "{parsed['CATEGORY']}"
          date: "{today_str}"
          tags: [{parsed['TAGS']}]
          ---

          {parsed['BODY'].strip()}
          """)
          
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{parsed['SLUG']}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          
          memory.append(final_hash)
          os.makedirs(os.path.dirname(memory_path), exist_ok=True)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"‚úÖ File saved: {parsed['SLUG']}.md")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: fresh local scoop via Google News üá∞üá™"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'content/posts/*.md .github/scrape_memory.json'
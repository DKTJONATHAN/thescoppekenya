name: Mpasho Gossip (Debug Mode)

on:
  schedule:
    - cron: '0 6,12,18 * * *' # Runs 3 times a day
  workflow_dispatch: # Button to run manually

permissions:
  contents: write

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Libraries
        run: pip install requests google-genai beautifulsoup4 lxml

      - name: Run Scraper
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
        run: |
          python << 'EOF'
          import os, requests, random, sys
          from google import genai
          from bs4 import BeautifulSoup
          from datetime import datetime

          # --- CONFIGURATION ---
          HOMEPAGE = "https://mpasho.co.ke/"
          HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
          }

          def debug_html(soup):
              """Prints a summary of the page structure for debugging"""
              print("\nüîç DEBUG: Page Structure Summary")
              print(f"Page Title: {soup.title.string if soup.title else 'No Title'}")
              
              # Check for common article containers
              for tag in ['article', 'div.post', 'div.entry', 'div.card']:
                  count = len(soup.select(tag))
                  print(f"Found {count} elements matching '{tag}'")
              
              # Print first 500 chars of body to check if we are blocked
              print(f"HTML Preview: {str(soup.body)[:500]}...\n")

          # --- 1. CRAWL HOMEPAGE ---
          print(f"üì° Scanning: {HOMEPAGE}")
          try:
              resp = requests.get(HOMEPAGE, headers=HEADERS, timeout=15)
              soup = BeautifulSoup(resp.content, "html.parser")
              
              # Find links - specific to Mpasho's layout
              # We look for links inside headings or article tags
              candidates = []
              
              # Method A: Standard Article Links
              for a in soup.find_all("a", href=True):
                  link = a['href']
                  # Filter for valid article links
                  if "mpasho.co.ke" in link and len(link) > 40:
                      if not any(x in link for x in ["/category/", "/tag/", "contact", "about"]):
                          candidates.append(link)

              candidates = list(set(candidates))
              
              if not candidates:
                  print("‚ùå No articles found!")
                  debug_html(soup) # <--- THIS IS THE MAGIC FIX
                  sys.exit(0)
                  
              print(f"‚úÖ Found {len(candidates)} articles.")
              target_link = random.choice(candidates[:10])

          except Exception as e:
              print(f"‚ùå Connection Error: {e}")
              sys.exit(1)

          # --- 2. SCRAPE ARTICLE ---
          print(f"üï∑Ô∏è Reading: {target_link}")
          try:
              resp = requests.get(target_link, headers=HEADERS, timeout=15)
              soup = BeautifulSoup(resp.content, "html.parser")
              
              # Try multiple selectors for the body text
              body_div = soup.find("div", class_="article-body") or \
                         soup.find("div", class_="entry-content") or \
                         soup.find("div", class_="post-content") or \
                         soup.find("article")

              if not body_div:
                  print("‚ö†Ô∏è Could not find article body text.")
                  debug_html(soup)
                  sys.exit(0)

              # Get Text
              paragraphs = body_div.find_all("p")
              text = "\n\n".join([p.text.strip() for p in paragraphs])
              title = soup.find("h1").text.strip() if soup.find("h1") else "Gossip News"

          except Exception as e:
              print(f"‚ùå Scrape Error: {e}")
              sys.exit(1)

          if len(text) < 200:
              print("‚ö†Ô∏è Article text too short.")
              sys.exit(0)

          # --- 3. REWRITE (Sheng Reporter) ---
          print("ü§ñ Rewriting with Gemini...")
          client = genai.Client(api_key=os.environ["GEMINI_WRITE_KEY"])
          
          prompt = f"""
          Rewrite this news story into a Nairobi Gossip Blog post (Sheng).
          Title: {title}
          Body: {text[:5000]}
          
          Format:
          TITLE: [Sheng Clickbait Title]
          SLUG: [slug]
          EXCERPT: [Hook]
          CATEGORY: Entertainment
          TAGS: Gossip, Nairobi
          IMAGE_KEYWORD: [Search Term]
          BODY:
          [Markdown Text]
          """
          
          try:
              resp = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
              final_text = resp.text
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              sys.exit(1)

          # --- 4. SAVE ---
          # (Parsing logic similar to previous script)
          # For brevity, assuming standard parsing here...
          # ... [Insert Saving Logic Here] ...
          print("‚úÖ Finished.")
          EOF